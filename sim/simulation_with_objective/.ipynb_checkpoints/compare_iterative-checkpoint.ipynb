{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8438efca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.8/site-packages (0.13.1)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.22.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/conda/lib/python3.8/site-packages (from seaborn) (3.5.2)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.3.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.33.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.0.8)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-clearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from numpy.polynomial.chebyshev import Chebyshev\n",
    "#%matplotlib notebook\n",
    "random_seed = 1\n",
    "np.random.seed(seed=1)\n",
    "torch.manual_seed(2)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-tourism",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.path.join(os.getcwd(), \"plot/\")\n",
    "today=datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8308a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ec6888",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FHatNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(FHatNetwork, self).__init__()\n",
    "        layers = []\n",
    "        if hidden_sizes == []:\n",
    "            self.model = nn.Linear(input_size, output_size)\n",
    "        else:\n",
    "            layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "            layers.append(nn.ReLU())\n",
    "            for i in range(1, len(hidden_sizes)):\n",
    "                layers.append(nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]))\n",
    "                layers.append(nn.ReLU())\n",
    "            layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "            self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the target network\n",
    "class TargetNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(TargetNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Define the hypernetwork\n",
    "class HyperNetwork(nn.Module):\n",
    "    def __init__(self, target_net, hyper_input_dim, hyper_hidden_dims):\n",
    "        super(HyperNetwork, self).__init__()\n",
    "        self.target_net = target_net\n",
    "        layers = [] \n",
    "        if hyper_hidden_dims == []:\n",
    "            layers.append(nn.Linear(hyper_input_dim, target_net.fc.weight.numel()))\n",
    "        else:\n",
    "            layers.append(nn.Linear(hyper_input_dim, hyper_hidden_dims[0]))\n",
    "            layers.append(nn.ReLU())\n",
    "            for i in range(1, len(hyper_hidden_dims)):\n",
    "                layers.append(nn.Linear(hyper_hidden_dims[i-1],hyper_hidden_dims[i]))\n",
    "            layers.append(nn.Linear(hyper_hidden_dims[-1], target_net.fc.weight.numel()))\n",
    "        \n",
    "        layers.append(nn.Tanh())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x, alpha):\n",
    "        alpha = alpha.unsqueeze(0)\n",
    "        params = self.model(alpha)\n",
    "        params = params.view(self.target_net.fc.weight.size())\n",
    "        self.target_net.fc.weight.data = params\n",
    "        return self.target_net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-spelling",
   "metadata": {},
   "source": [
    "**Preliminaries** In the following, the classes are defined to initiate the aggregation functions $\\rho_{\\alpha}$ and the aggregated risk minimization (ARM) optimization for a simple 1D and 2D regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-jungle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class aggregation_function:    \n",
    "    \"\"\" This class aggregates the risks. \"\"\"\n",
    "    def __init__(self, name:str):\n",
    "        self.name = name\n",
    "    def aggregate(self, risks, alpha) -> float:\n",
    "        if self.name == 'cvar':\n",
    "            return self.cvar(risks, alpha)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Currently, only CVaR is implemented.\")\n",
    "    def cvar(self, risks, alpha) -> float:\n",
    "        var = torch.quantile(risks,alpha, interpolation='linear')\n",
    "        cvar = risks[risks > var].mean()\n",
    "        return cvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-surrey",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARM_Regression:\n",
    "    def __init__(self, name, experiment=\"1D_linear\"):      \n",
    "        self.aggregator = aggregation_function(name=name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    def compute_cvar_h(self, alpha, h, env_dict):\n",
    "        with torch.no_grad():\n",
    "            risks = []\n",
    "            for e in env_dict.keys():\n",
    "                output = h(env_dict[e]['x'].to(self.device), alpha.to(self.device))\n",
    "                risks.append(loss_fn(env_dict[e]['y'].to(self.device),output))\n",
    "            risks = torch.stack(risks)\n",
    "            cvar = self.aggregator.aggregate(risks, alpha)\n",
    "        return cvar\n",
    "    \n",
    "    def fit_h(self, h, env_dict, alphas):        \n",
    "        learning_rate = 0.01\n",
    "        num_epochs= 200\n",
    "        d = env_dict[0]['x'].shape[1]\n",
    "        loss_fn = torch.nn.MSELoss()        \n",
    "        optimizer = torch.optim.Adam(h.parameters(), lr=learning_rate)\n",
    "        scheduler = StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "        for epoch in range(num_epochs):\n",
    "            for alpha in alphas:\n",
    "                risks = []\n",
    "                for e in env_dict.keys():\n",
    "                    risks.append(loss_fn(env_dict[e]['y'].to(self.device),h(env_dict[e]['x'].to(self.device),alpha.to(self.device)))) \n",
    "                risks = torch.stack(risks)\n",
    "                cvar = self.aggregator.aggregate(risks, alpha)\n",
    "                cvar.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {cvar.item()}\")\n",
    "        return \n",
    "    \n",
    "    def iterative_implementation(self, h, cheb_approx, env_dict, loss_fn):\n",
    "        tests = np.random.uniform(low=0, high=1, size=100)\n",
    "        tests = torch.tensor(tests, dtype=torch.float32).to(device)\n",
    "        for test in tests:\n",
    "            cvar_cheb = cheb_approx(test)\n",
    "            cvar_h = self.compute_cvar_h(test, h, env_dict)\n",
    "            if cvar_cheb < cvar_h:\n",
    "                return test\n",
    "        return \"exit\"\n",
    "    \n",
    "    def fit_h_chebyshev(self, h, env_dict, start, budget):\n",
    "        learning_rate = 0.01\n",
    "        num_epochs= 100\n",
    "        d = env_dict[0]['x'].shape[1]\n",
    "        loss_fn = torch.nn.MSELoss()        \n",
    "        optimizer = torch.optim.Adam(h.parameters(), lr=learning_rate)\n",
    "        scheduler = StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "        alphas = np.cos((np.pi * (2 * np.arange(1, budget + 1) - 1)) / (2 * budget)) / 2 + 0.5\n",
    "        alphas = torch.tensor(alphas, dtype=torch.float32).to(self.device) \n",
    "        while (budget):\n",
    "            self.fit_h(h, env_dict, alphas)\n",
    "            cvar_values = np.array([self.compute_cvar_h(alpha, h, env_dict) for alpha in alphas])\n",
    "            cheb_approx = Chebyshev.fit(alphas, cvar_values, deg=budget-1)\n",
    "            alpha = self.iterative_implementation(h, cheb_approx, env_dict, loss_fn)\n",
    "            if alpha == \"exit\":\n",
    "                return \n",
    "            else:\n",
    "                alphas = torch.cat((alphas,alpha.unsqueeze(0)), dim=0)\n",
    "            budget=budget-1\n",
    "        return \n",
    "    \n",
    "    def monte_carlo_h(self, h, env_dict, loss_fn, num_samples=10):\n",
    "        cvar_estimates = []\n",
    "        alphas = np.random.uniform(low=0, high=1, size=num_samples)\n",
    "        alphas = torch.tensor(alphas, dtype=torch.float32).to(self.device)\n",
    "        for alpha in alphas:\n",
    "            risks = []\n",
    "            for e in env_dict.keys():\n",
    "                risks.append(loss_fn(env_dict[e]['y'].to(device),h(env_dict[e]['x'].to(device), alpha)))\n",
    "            risks = torch.stack(risks)\n",
    "            cvar_sample = self.aggregator.aggregate(risks, alpha)\n",
    "            cvar_estimates.append(cvar_sample)\n",
    "        cvar_estimates = torch.stack(cvar_estimates)\n",
    "        average_cvar = torch.mean(cvar_estimates)\n",
    "        return average_cvar     \n",
    "    \n",
    "    def fit_h_new(self, h, env_dict):\n",
    "        learning_rate = 0.01\n",
    "        num_epochs= 100\n",
    "        d = env_dict[0]['x'].shape[1]\n",
    "        loss_fn = torch.nn.MSELoss()        \n",
    "        optimizer = torch.optim.Adam(h.parameters(), lr=learning_rate)\n",
    "        scheduler = StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "        for epoch in range(num_epochs):\n",
    "            avg_cvar = self.monte_carlo_h(h, env_dict, loss_fn, num_samples=100)\n",
    "            avg_cvar.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_cvar.item()}\")\n",
    "        return   \n",
    "    \n",
    "    def fit_f(self, f, env_dict, alpha):        \n",
    "        learning_rate = 0.01\n",
    "        num_epochs= 200\n",
    "        d = env_dict[0]['x'].shape[1]\n",
    "        loss_fn = torch.nn.MSELoss()        \n",
    "        optimizer = torch.optim.Adam(f.parameters(), lr=learning_rate)\n",
    "        scheduler = StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "        for epoch in range(num_epochs):\n",
    "            risks = torch.stack([loss_fn(env_dict[e]['y'].to(self.device),f(env_dict[e]['x'].to(self.device))) for e in env_dict.keys()])\n",
    "            cvar = self.aggregator.aggregate(risks, alpha)\n",
    "            cvar.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {cvar.item()}\")\n",
    "        return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-genome",
   "metadata": {},
   "source": [
    "**Experiment 1** We assume the following *linear* data generation process $$Y(X) = X*\\theta_{e}+\\epsilon$$ and *nonlinear* data generation process $$Y(X) = sin(X*\\theta_{e})+\\epsilon$$.\n",
    "\n",
    "**Experiment 1A** Assume a linear model $Y_{e}=\\theta_{e}X+\\epsilon$, where $X \\sim \\mathcal{N}(2,0.2)$ and $\\epsilon\\sim \\mathcal{N}(0,0.1)$. We simulate different environments by drawing $\\theta$ from a beta distribution $Beta(0.1,0.2)$. In total, we generate for 25 environments 100 observations each.\n",
    "\n",
    "**Experiment 1B** Assume the same setting as in Experiment 1, however, in contrast, we simulate different environments by drawing $\\theta$ from a uniform distribution $\\ \\mathcal{U}(0,1)$. In total, we generate for 25 environments 100 observations each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator:\n",
    "    \n",
    "    \"\"\" This class generates the simulation data. \"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self, envs_train = 25, envs_test = 25, \n",
    "                 size_train = 1000, size_test= 100, \n",
    "                 theta_dist=\"uniform\",\n",
    "                 dim=1):\n",
    "        \n",
    "        self.envs_train = envs_train\n",
    "        self.envs_test = envs_test\n",
    "        self.size_train = size_train\n",
    "        self.size_test = size_test\n",
    "        self.theta_dist = theta_dist\n",
    "        self.dim = dim \n",
    "        \n",
    "    def generate(self) -> dict:           \n",
    "        env_list_train = [f'e_{i}' for i in range(1,self.envs_train+1,1)]\n",
    "        env_dict_train = dict(list(enumerate(env_list_train)))\n",
    "        env_list_test  = [f'e_{i}' for i in range(1,self.envs_test+1,1)]\n",
    "        env_dict_test  = dict(list(enumerate(env_list_test)))\n",
    "        \n",
    "        \n",
    "        for e_train in env_dict_train.keys():\n",
    "            if self.theta_dist == \"uniform\": \n",
    "                distribution = dist.Uniform(0, 1)\n",
    "            else:\n",
    "                distribution = dist.Beta(0.1, 0.2)\n",
    "            theta_true = distribution.sample((self.dim,1))\n",
    "            x_train = dist.normal.Normal(loc=1.0/self.dim, scale=0.5).sample((self.size_train,self.dim))\n",
    "            noise_train = dist.normal.Normal(loc=0, scale=0.05).sample((self.size_train,self.dim))\n",
    "            y_train = (1.0/math.sqrt(self.dim))*x_train@theta_true + noise_train\n",
    "            env_dict_train[e_train] = {'x': x_train,'y': y_train,'theta_true': theta_true}\n",
    "            \n",
    "        for e_test in env_dict_test.keys():\n",
    "            if self.theta_dist == \"uniform\": \n",
    "                distribution = dist.Uniform(0, 1)\n",
    "            else:\n",
    "                distribution = dist.Beta(0.1, 0.2)\n",
    "            theta_true = distribution.sample((self.dim,1))\n",
    "            x_test = dist.normal.Normal(loc=1.0/self.dim, scale=0.5).sample((self.size_test,self.dim))\n",
    "            noise_test = dist.normal.Normal(loc=0, scale=0.05).sample((self.size_test,self.dim))\n",
    "            y_test = (1.0/math.sqrt(self.dim))*x_test@theta_true + noise_test\n",
    "            env_dict_test[e_test] = {'x': x_test,'y': y_test,'theta_true': theta_true}\n",
    "            \n",
    "        return env_dict_train, env_dict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17c1c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the target network and hypernetwork\n",
    "dim = 1\n",
    "target_net = TargetNetwork(input_dim=dim, output_dim=1)\n",
    "h = HyperNetwork(target_net, hyper_input_dim=1, hyper_hidden_dims=[10]).to(device)\n",
    "print(h.target_net.fc.weight.data.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-instrument",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Intitialize the experiment and generate the data\n",
    "envs_train, envs_test = 250, 250\n",
    "size_train, size_test = 10000 , 100\n",
    "theta_dist=\"beta\" \n",
    "rho=\"cvar\"\n",
    "\n",
    "generator = data_generator(envs_train, envs_test, size_train, size_test, theta_dist, dim)\n",
    "data_dict_train, data_dict_test = generator.generate()\n",
    "# Define some example dimensions\n",
    "generator = data_generator(envs_train, envs_test, size_train, size_test, theta_dist, dim)\n",
    "data_dict_train, data_dict_test = generator.generate()\n",
    "fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
    "\n",
    "X_train = [data_dict_train[e]['x'].ravel() for e in data_dict_train.keys()]\n",
    "X_train = np.array([item for sublist in X_train for item in sublist]).reshape(-1,1)\n",
    "Y_train = [data_dict_train[e]['y'] for e in data_dict_train.keys()]\n",
    "Y_train = np.array([item for sublist in Y_train for item in sublist]).reshape(-1,1)\n",
    "\n",
    "ax.scatter(y=Y_train, x=X_train, s=3, c=\"grey\", alpha=0.05, label=\"Training envs\")\n",
    "ax.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81499292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the ARM regression task\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "results = pd.DataFrame(columns = [\"alpha\", \"cvar\"])\n",
    "fig, ax = plt.subplots(1, 1, sharey=True, figsize=(8,6))\n",
    "rg = torch.arange(0,1,0.05)\n",
    "diverging_colors = sns.color_palette(\"RdBu\", len(rg))\n",
    "ARM_Regression(name=rho).fit_h_chebyshev(h, data_dict_train, 10, 10) \n",
    "h.eval()\n",
    "j=1\n",
    "for alpha in tqdm(rg):\n",
    "    with torch.no_grad():\n",
    "        risks = torch.stack([loss_fn(data_dict_train[e]['y'].to(device), h(data_dict_train[e]['x'].to(device), alpha.to(device))) for e in data_dict_train.keys()])\n",
    "        cvar_emp = aggregation_function(name=\"cvar\").aggregate(risks.cpu(), alpha.cpu())     \n",
    "        sns.kdeplot(risks.cpu().numpy(), ax=ax,color=diverging_colors[len(rg)-j], label=str(alpha.item()))\n",
    "        print(\"alpha: \", alpha, \" CVaR: \",cvar_emp.item(), \"param: \",h.target_net.fc.weight.data.item() )\n",
    "    results.at[int(alpha.item()*100), \"alpha\"] = alpha\n",
    "    results.at[int(alpha.item()*100), \"cvar\"] = cvar_emp\n",
    "    j+=1\n",
    "ax.legend(bbox_to_anchor=(1.02, 1.02), title=r\"$\\alpha$\", loc='upper left')\n",
    "ax.set_title(r\"Dist of risks on train for optimization of $h_\\beta(x,\\alpha)$\")\n",
    "norm = plt.Normalize(0.05, 0.95)\n",
    "sm = plt.cm.ScalarMappable(cmap=\"RdBu_r\", norm=norm)\n",
    "sm.set_array([])\n",
    "ax.get_legend().remove()\n",
    "ax.figure.colorbar(sm).set_label(label= r'$\\alpha$',labelpad=5,size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e7017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(h, 'hypernetwork_entire_model.pth')\n",
    "#h = torch.load('hypernetwork_entire_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
