{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8438efca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.8/site-packages (0.13.2)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.3.5)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.22.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/conda/lib/python3.8/site-packages (from seaborn) (3.5.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.0.8)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.0.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.33.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (21.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.1.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.9)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (8.3.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.29)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.5.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.12.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: executing in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: SciencePlots in /opt/conda/lib/python3.8/site-packages (2.1.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from SciencePlots) (3.5.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from matplotlib->SciencePlots) (1.22.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->SciencePlots) (1.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->SciencePlots) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->SciencePlots) (4.33.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->SciencePlots) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib->SciencePlots) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->SciencePlots) (9.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->SciencePlots) (3.0.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->SciencePlots) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn\n",
    "!pip install ipywidgets\n",
    "!pip install SciencePlots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "owned-clearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from numpy.polynomial.chebyshev import Chebyshev\n",
    "from scipy.stats import beta\n",
    "#%matplotlib notebook\n",
    "random_seed = 0\n",
    "np.random.seed(seed=random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "infinite-tourism",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.path.join(os.getcwd(), \"plot/\")\n",
    "today=datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8308a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77ec6888",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FHatNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(FHatNetwork, self).__init__()\n",
    "        layers = []\n",
    "        if hidden_sizes == []:\n",
    "            self.model = nn.Linear(input_size, output_size)\n",
    "        else:\n",
    "            layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "            layers.append(nn.ReLU())\n",
    "            for i in range(1, len(hidden_sizes)):\n",
    "                layers.append(nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]))\n",
    "                layers.append(nn.ReLU())\n",
    "            layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "            self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class GTLearner(nn.Module):\n",
    "    def _init_(self):\n",
    "        super(GTLearner, self).__init__()\n",
    "        self.random_weights = torch.dist.Uniform(-1.1,1.1).sample((100,1))\n",
    "        self.network = nn.Linear(100,1)\n",
    "    def forward(self, x):\n",
    "        return self.network(self.random_weights*x)\n",
    "    \n",
    "# Define the hypernetwork\n",
    "class HyperNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(HyperNetwork, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hyper_layer = FHatNetwork(1,[],input_dim)\n",
    "        \n",
    "    def forward(self, x, alpha):\n",
    "        alpha = alpha.unsqueeze(0)\n",
    "        return x@torch.tanh(self.hyper_layer(alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-spelling",
   "metadata": {},
   "source": [
    "**Preliminaries** In the following, the classes are defined to initiate the aggregation functions $\\rho_{\\alpha}$ and the aggregated risk minimization (ARM) optimization for a simple 1D and 2D regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "empirical-jungle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Quantile(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, risks, alpha):\n",
    "        ctx.save_for_backward(risks, alpha)\n",
    "        return torch.quantile(risks, alpha)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        risks, alpha = ctx.saved_tensors\n",
    "        diff = 1e-5\n",
    "        grad_risks = grad_output * 0\n",
    "        if alpha < diff:\n",
    "            local_grad_alpha = (torch.quantile(risks, alpha+diff) - torch.quantile(risks, alpha))/diff\n",
    "        elif alpha + diff > 1.0:\n",
    "            local_grad_alpha = (torch.quantile(risks, alpha) - torch.quantile(risks, alpha-diff))/diff\n",
    "        else:\n",
    "            local_grad_alpha = (torch.quantile(risks, alpha+diff) - torch.quantile(risks, alpha-diff)) / (2.0 * diff)\n",
    "        grad_alpha = grad_output * local_grad_alpha\n",
    "        return grad_risks, grad_alpha\n",
    "    \n",
    "class aggregation_function:    \n",
    "    \"\"\" This class aggregates the risks. \"\"\"\n",
    "    def __init__(self, name:str):\n",
    "        self.name = name\n",
    "    def aggregate(self, risks, alpha) -> float:\n",
    "        if self.name == 'cvar':\n",
    "            return self.cvar(risks, alpha)\n",
    "        elif self.name == 'cvar-full':\n",
    "            return self.cvar_full(risks, alpha)\n",
    "        elif self.name == 'cvar-diff':\n",
    "            return self.cvar_diff(risks, alpha)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Currently, only CVaR is implemented.\")\n",
    "    \n",
    "    def cvar_full(self, risks, alpha) -> float:\n",
    "        var = torch.quantile(risks, alpha)\n",
    "        cvar_plus = risks[risks >= var].mean()\n",
    "        lambda_alpha = ((risks <= var_alpha).mean() - alpha) / (1 - alpha)\n",
    "        cvar_alpha = lambda_alpha * var_alpha + (1 - lambda_alpha) * cvar_alpha_plus\n",
    "        return cvar\n",
    "    \n",
    "    def cvar_diff(self, risks, base_alpha) -> float:\n",
    "        number_of_points = 5\n",
    "        alphas = [(1-base_alpha)*(i/number_of_points)+base_alpha for i in range(number_of_points)]\n",
    "        quantiles = torch.stack([Quantile.apply(risks, alpha) for alpha in alphas])\n",
    "        return quantiles.mean()\n",
    "    \n",
    "    def cvar(self, risks, alpha) -> float:\n",
    "        var = torch.quantile(risks,alpha, interpolation='linear')\n",
    "        cvar = risks[risks >= var].mean()\n",
    "        return cvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "859ee193",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IcdfBetaScaler(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, a, b):\n",
    "        ctx.save_for_backward(x, a, b)\n",
    "        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        return torch.tensor(beta.ppf(x.item(), a.item(), b.item())).float().to(device)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, a, b = ctx.saved_tensors\n",
    "        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        diff = 1e-5\n",
    "        x = x.item()\n",
    "        a = a.item()\n",
    "        b = b.item()\n",
    "        if x < diff:\n",
    "            local_grad_x = torch.tensor([(beta.ppf(x+diff, a, b) - beta.ppf(x, a, b)) / diff]).float()\n",
    "        elif x + diff > 1.0:\n",
    "            local_grad_x = torch.tensor([(beta.ppf(x, a, b) - beta.ppf(x-diff, a, b)) / diff]).float()\n",
    "        else:\n",
    "            local_grad_x = torch.tensor([(beta.ppf(x+diff, a, b) - beta.ppf(x-diff, a, b)) / (2.0 * diff)]).float()\n",
    "\n",
    "        if a < diff:\n",
    "            local_grad_a = torch.tensor([(beta.ppf(x, a+diff, b) - beta.ppf(x, a, b)) / diff]).float()\n",
    "        else:\n",
    "            local_grad_a = torch.tensor([(beta.ppf(x, a+diff, b) - beta.ppf(x, a-diff, b)) / (2.0 * diff)]).float()\n",
    "\n",
    "        if b < diff:\n",
    "            local_grad_b = torch.tensor([(beta.ppf(x, a, b+diff) - beta.ppf(x, a, b)) / diff]).float()\n",
    "        else:\n",
    "            local_grad_b = torch.tensor([(beta.ppf(x, a, b+diff) - beta.ppf(x, a, b-diff)) / (2.0 * diff)]).float()\n",
    "        grad_x = grad_output * local_grad_a.to(device)\n",
    "        grad_a = grad_output * local_grad_a.to(device)\n",
    "        grad_b = grad_output * local_grad_b.to(device)\n",
    "        return grad_x, grad_a, grad_b\n",
    "    \n",
    "class Pareto_distribution:\n",
    "    def __init__(self, env_dict):\n",
    "        self.env_dict = env_dict\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.aggregator = aggregation_function(name=\"cvar-diff\")\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def compute_norm(self, model):\n",
    "        # Calculate norm of gradients\n",
    "        total_norm = 0\n",
    "        for param in model.parameters():\n",
    "            param_norm = param.grad.data.norm(2)\n",
    "            total_norm += param_norm**2\n",
    "        total_norm = total_norm ** 0.5\n",
    "        return total_norm\n",
    "    \n",
    "    def aggregated_objective(self, model, a, b, num_samples=5):\n",
    "        ### reparameterization needed here.\n",
    "        uniform_samples = dist.Uniform(0,1).sample((num_samples,))\n",
    "        alphas = []\n",
    "        for unif_sample in uniform_samples:\n",
    "            t_unif = torch.tensor(unif_sample, requires_grad=True, device=self.device)\n",
    "            alphas.append(IcdfBetaScaler.apply(t_unif, a, b))\n",
    "        cvar_estimates = []\n",
    "        for alpha in alphas:\n",
    "            risks = []\n",
    "            for e in self.env_dict.keys():\n",
    "                x, y = self.env_dict[e]['x'].to(self.device), self.env_dict[e]['y'].to(self.device) \n",
    "                x.requires_grad, y.requires_grad = False, False\n",
    "                risks.append(self.loss_fn(y, model(x,alpha)))\n",
    "            risks = torch.stack(risks).detach()\n",
    "            cvar_estimates.append(self.aggregator.aggregate(risks, alpha))\n",
    "        cvar_estimates = torch.stack(cvar_estimates)\n",
    "        average_cvar = torch.mean(cvar_estimates)\n",
    "        return average_cvar\n",
    "    \n",
    "    def optimize(self, model):\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        a = torch.tensor([1.0], requires_grad=True, device=self.device, dtype=torch.float32)\n",
    "        b = torch.tensor([1.0], requires_grad=True, device=self.device, dtype=torch.float32)\n",
    "        optimizer_dist = torch.optim.Adam([a, b], lr=0.01)\n",
    "        num_epochs = 10\n",
    "        for epoch in range(num_epochs):\n",
    "            avg_cvar = self.aggregated_objective(model, a, b)\n",
    "            avg_cvar.backward()\n",
    "            optimizer_dist.step()\n",
    "            optimizer_dist.zero_grad()\n",
    "        return a.detach().item(), b.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "thick-surrey",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARM_Regression:\n",
    "    def __init__(self, name, experiment=\"1D_linear\"):      \n",
    "        self.aggregator = aggregation_function(name=name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    def compute_cvar_h(self, alpha, h, env_dict):\n",
    "        loss_fn = torch.nn.MSELoss()        \n",
    "        risks = []\n",
    "        for e in env_dict.keys():\n",
    "            output = h(env_dict[e]['x'].to(self.device), alpha.to(self.device))\n",
    "            risks.append(loss_fn(output,env_dict[e]['y'].to(self.device)))\n",
    "        risks = torch.stack(risks)\n",
    "        cvar = self.aggregator.aggregate(risks, alpha)\n",
    "        return cvar\n",
    "    \n",
    "    def fit_h(self, h, env_dict, a, b, num_epochs=30):\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        alphas = np.random.beta(a=a, b=b, size=30)\n",
    "        alphas = torch.tensor(alphas, dtype=torch.float32).to(self.device)\n",
    "        learning_rate = 0.1\n",
    "        optimizer = torch.optim.Adam(h.parameters(), lr=learning_rate)\n",
    "        scheduler = StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "        for epoch in range(num_epochs):\n",
    "            avg_cvar = torch.mean(torch.stack([self.compute_cvar_h(alpha, h, env_dict) for alpha in alphas]))\n",
    "            avg_cvar.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_cvar.item()}\")\n",
    "        return \n",
    "    \n",
    "    def fit_f(self, f, env_dict, alpha):        \n",
    "        learning_rate = 0.1\n",
    "        num_epochs= 100\n",
    "        d = env_dict[0]['x'].shape[1]\n",
    "        loss_fn = torch.nn.MSELoss()        \n",
    "        optimizer = torch.optim.Adam(f.parameters(), lr=learning_rate)\n",
    "        scheduler = StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "        for epoch in range(num_epochs):\n",
    "            risks = []\n",
    "            for e in env_dict.keys():\n",
    "                x, y = env_dict[e]['x'].to(self.device), env_dict[e]['y'].to(self.device) \n",
    "                risks.append(loss_fn(f(x),y))\n",
    "            risks = torch.stack(risks)\n",
    "            cvar = self.aggregator.aggregate(risks, alpha)\n",
    "            cvar.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {cvar.item()}\")\n",
    "        return \n",
    "    \n",
    "    def fit_h_as_f(self, h, env_dict, alpha): \n",
    "        t_alpha = torch.tensor([alpha]).to(self.device)\n",
    "        learning_rate = 0.1\n",
    "        num_epochs= 100\n",
    "        d = env_dict[0]['x'].shape[1]\n",
    "        loss_fn = torch.nn.MSELoss()        \n",
    "        optimizer = torch.optim.Adam(h.parameters(), lr=learning_rate)\n",
    "        scheduler = StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "        for epoch in range(num_epochs):\n",
    "            risks = []\n",
    "            for e in env_dict.keys():\n",
    "                x, y = env_dict[e]['x'].to(self.device), env_dict[e]['y'].to(self.device) \n",
    "                risks.append(loss_fn(h(x,t_alpha),y))\n",
    "            risks = torch.stack(risks)\n",
    "            cvar = self.aggregator.aggregate(risks, alpha)\n",
    "            cvar.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {cvar.item()}\")\n",
    "        return\n",
    "    \n",
    "    def fit_h_pareto(self, h, env_dict, num_epochs=30):\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        learning_rate = 0.1\n",
    "        optimizer = torch.optim.Adam(h.parameters(), lr=learning_rate)\n",
    "        scheduler = StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "        p_min = Pareto_distribution(env_dict)\n",
    "        for epoch in range(num_epochs):\n",
    "            a, b = p_min.optimize(copy.deepcopy(h))\n",
    "            alphas = np.random.beta(a, b, size=5) \n",
    "            alphas = torch.tensor(alphas, dtype=torch.float32).to(self.device)\n",
    "            avg_cvar = torch.mean(torch.stack([self.compute_cvar_h(alpha, h, env_dict) for alpha in alphas]))\n",
    "            avg_cvar.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_cvar.item()}\")\n",
    "        return  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-genome",
   "metadata": {},
   "source": [
    "**Experiment 1** We assume the following *linear* data generation process $$Y(X) = X*\\theta_{e}+\\epsilon$$ and *nonlinear* data generation process $$Y(X) = sin(X*\\theta_{e})+\\epsilon$$.\n",
    "\n",
    "**Experiment 1A** Assume a linear model $Y_{e}=\\theta_{e}X+\\epsilon$, where $X \\sim \\mathcal{N}(2,0.2)$ and $\\epsilon\\sim \\mathcal{N}(0,0.1)$. We simulate different environments by drawing $\\theta$ from a beta distribution $Beta(0.1,0.2)$. In total, we generate for 25 environments 100 observations each.\n",
    "\n",
    "**Experiment 1B** Assume the same setting as in Experiment 1, however, in contrast, we simulate different environments by drawing $\\theta$ from a uniform distribution $\\ \\mathcal{U}(0,1)$. In total, we generate for 25 environments 100 observations each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pharmaceutical-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator:\n",
    "    \n",
    "    \"\"\" This class generates the simulation data. \"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self, envs_train = 25, envs_test = 25, \n",
    "                 size_train = 1000, size_test= 100, \n",
    "                 theta_dist=\"uniform\",\n",
    "                 dim=1):\n",
    "        \n",
    "        self.envs_train = envs_train\n",
    "        self.envs_test = envs_test\n",
    "        self.size_train = size_train\n",
    "        self.size_test = size_test\n",
    "        self.theta_dist = theta_dist\n",
    "        self.dim = dim \n",
    "        \n",
    "    def generate(self) -> dict:           \n",
    "        env_list_train = [f'e_{i}' for i in range(1,self.envs_train+1,1)]\n",
    "        env_dict_train = dict(list(enumerate(env_list_train)))\n",
    "        env_list_test  = [f'e_{i}' for i in range(1,self.envs_test+1,1)]\n",
    "        env_dict_test  = dict(list(enumerate(env_list_test)))\n",
    "        \n",
    "        \n",
    "        for e_train in env_dict_train.keys():\n",
    "            if self.theta_dist == \"uniform\": \n",
    "                distribution = dist.Uniform(0, 1)\n",
    "            elif self.theta_dist == \"beta\":\n",
    "                distribution = dist.Beta(0.1, 0.2)\n",
    "            else:\n",
    "                distribution1 = dist.Uniform(-1.1, -1)\n",
    "                distribution2 = dist.Uniform(1, 1.1)\n",
    "                decider = dist.Bernoulli(torch.tensor([0.5])).sample()\n",
    "                distribution = distribution1 if decider==1 else distribution2\n",
    "            theta_true = distribution.sample((self.dim,1))\n",
    "            x_train = dist.normal.Normal(loc=1.0/self.dim, scale=0.5).sample((self.size_train,self.dim))\n",
    "            noise_train = dist.normal.Normal(loc=0, scale=0.1).sample((self.size_train,self.dim))\n",
    "            y_train = (1.0/math.sqrt(self.dim))*x_train@theta_true + noise_train\n",
    "            env_dict_train[e_train] = {'x': x_train,'y': y_train,'theta_true': theta_true}\n",
    "            \n",
    "        for e_test in env_dict_test.keys():\n",
    "            if self.theta_dist == \"uniform\": \n",
    "                distribution = dist.Uniform(0, 1)\n",
    "            elif self.theta_dist == \"beta\":\n",
    "                distribution = dist.Beta(0.1, 0.2)\n",
    "            else:\n",
    "                distribution1 = dist.Uniform(-1.1, -1)\n",
    "                distribution2 = dist.Uniform(1, 1.1)\n",
    "                decider = dist.Bernoulli(torch.tensor([0.5])).sample()\n",
    "                distribution = distribution1 if decider==1 else distribution2\n",
    "            theta_true = distribution.sample((self.dim,1))\n",
    "            x_test = dist.normal.Normal(loc=1.0/self.dim, scale=0.5).sample((self.size_test,self.dim))\n",
    "            noise_test = dist.normal.Normal(loc=0, scale=0.1).sample((self.size_test,self.dim))\n",
    "            y_test = (1.0/math.sqrt(self.dim))*x_test@theta_true + noise_test\n",
    "            env_dict_test[e_test] = {'x': x_test,'y': y_test,'theta_true': theta_true}\n",
    "            \n",
    "        return env_dict_train, env_dict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "general-instrument",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Intitialize the experiment and generate the data\n",
    "dim = 1\n",
    "envs_train, envs_test = 5, 250\n",
    "size_train, size_test = 100 , 100\n",
    "theta_dist=\"mixed\" \n",
    "rho=\"cvar\"\n",
    "\n",
    "generator = data_generator(envs_train, envs_test, size_train, size_test, theta_dist, dim)\n",
    "data_dict_train, data_dict_test = generator.generate()\n",
    "# Define some example dimensions\n",
    "generator = data_generator(envs_train, envs_test, size_train, size_test, theta_dist, dim)\n",
    "data_dict_train, data_dict_test = generator.generate()\n",
    "fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
    "\n",
    "X_train = [data_dict_train[e]['x'].ravel() for e in data_dict_train.keys()]\n",
    "X_train = np.array([item for sublist in X_train for item in sublist]).reshape(-1,1)\n",
    "Y_train = [data_dict_train[e]['y'] for e in data_dict_train.keys()]\n",
    "Y_train = np.array([item for sublist in Y_train for item in sublist]).reshape(-1,1)\n",
    "\n",
    "ax.scatter(y=Y_train, x=X_train, s=3, c=\"grey\", alpha=0.05, label=\"Training envs\")\n",
    "ax.legend(loc='upper left')\n",
    "fig.savefig(\"env.pdf\", format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "203cce92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "def fixed(data_dict_train, data_dict_test, dim, alphas):\n",
    "    h_pareto = HyperNetwork(dim).to(device)\n",
    "    ARM_Regression(name=rho).fit_h_pareto(h_pareto, data_dict_train)\n",
    "    print(\"Pareto Trained\")\n",
    "    \n",
    "    h_2_2 = HyperNetwork(dim).to(device)\n",
    "    ARM_Regression(name=rho).fit_h(h_2_2, data_dict_train, 2,2)\n",
    "    print(\"Beta 2 2 Trained\")  \n",
    "    h_5_1 = HyperNetwork(dim).to(device)\n",
    "    ARM_Regression(name=rho).fit_h(h_5_1, data_dict_train, 5,1)\n",
    "    print(\"Beta 5 1 Trained\")\n",
    "    h_1_1 = HyperNetwork(dim).to(device)\n",
    "    ARM_Regression(name=rho).fit_h(h_1_1, data_dict_train, 1,1)\n",
    "    print(\"Beta 1 1 Trained\")\n",
    "    \n",
    "    \n",
    "    f = FHatNetwork(dim, [],1).to(device)\n",
    "    iids = []\n",
    "    for alpha in alphas:\n",
    "        ARM_Regression(name=rho).fit_f(f, data_dict_train, alpha)\n",
    "        iids.append(copy.deepcopy(f))\n",
    "        \n",
    "    # gt corresponds to infinite data regime this is why we set data dict test as input to simulate it\n",
    "    f = GTLearner().to(device)\n",
    "    groundtruths = []\n",
    "    for alpha in alphas:\n",
    "        ARM_Regression(name=rho).fit_f(f, data_dict_test, alpha)\n",
    "        groundtruths.append(copy.deepcopy(f))\n",
    "    return groundtruths, iids, [h_2_2, h_5_1, h_1_1, h_pareto]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63c2528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(true_alpha, model, data_dict_test):\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        train_risks = []\n",
    "        for e in data_dict_test.keys():\n",
    "            x, y = data_dict_test[e]['x'].to(device), data_dict_test[e]['y'].to(device)\n",
    "            train_risks.append(loss_fn(model(x, torch.tensor(true_alpha).to(device)).cpu(), y.cpu())) \n",
    "        train_risks = torch.stack(train_risks)\n",
    "    cvar = aggregation_function(name=\"cvar\").aggregate(train_risks, true_alpha)\n",
    "    return cvar\n",
    "def execute_gt(true_alpha, model, data_dict_test):\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        train_risks = []\n",
    "        for e in data_dict_test.keys():\n",
    "            x, y = data_dict_test[e]['x'].to(device), data_dict_test[e]['y'].to(device)\n",
    "            train_risks.append(loss_fn(model(x).cpu(), y.cpu())) \n",
    "        train_risks = torch.stack(train_risks)\n",
    "    cvar = aggregation_function(name=\"cvar\").aggregate(train_risks, true_alpha)\n",
    "    return cvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e364615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 2.1944987773895264\n",
      "Epoch [2/30], Loss: 1.9439681768417358\n",
      "Epoch [3/30], Loss: 1.6569541692733765\n",
      "Epoch [4/30], Loss: 1.5984855890274048\n",
      "Epoch [5/30], Loss: 1.5212867259979248\n",
      "Epoch [6/30], Loss: 1.681209921836853\n",
      "Epoch [7/30], Loss: 1.6923885345458984\n",
      "Epoch [8/30], Loss: 1.5493242740631104\n",
      "Epoch [9/30], Loss: 1.5008344650268555\n",
      "Epoch [10/30], Loss: 1.541630506515503\n",
      "Epoch [11/30], Loss: 1.558293342590332\n",
      "Epoch [12/30], Loss: 1.5778844356536865\n",
      "Epoch [13/30], Loss: 1.6384696960449219\n",
      "Epoch [14/30], Loss: 1.5412975549697876\n",
      "Epoch [15/30], Loss: 1.5661652088165283\n",
      "Epoch [16/30], Loss: 1.5780519247055054\n",
      "Epoch [17/30], Loss: 1.553126573562622\n",
      "Epoch [18/30], Loss: 1.5184152126312256\n",
      "Epoch [19/30], Loss: 1.5176388025283813\n",
      "Epoch [20/30], Loss: 1.546103596687317\n",
      "Epoch [21/30], Loss: 1.5981690883636475\n",
      "Epoch [22/30], Loss: 1.5220491886138916\n",
      "Epoch [23/30], Loss: 1.5332527160644531\n",
      "Epoch [24/30], Loss: 1.5411124229431152\n",
      "Epoch [25/30], Loss: 1.5522230863571167\n",
      "Epoch [26/30], Loss: 1.514976978302002\n",
      "Epoch [27/30], Loss: 1.4919745922088623\n",
      "Epoch [28/30], Loss: 1.5367473363876343\n",
      "Epoch [29/30], Loss: 1.6117961406707764\n",
      "Epoch [30/30], Loss: 1.5711288452148438\n",
      "Pareto Trained\n",
      "Epoch [10/30], Loss: 1.6922972202301025\n",
      "Epoch [20/30], Loss: 1.6132045984268188\n",
      "Epoch [30/30], Loss: 1.5862947702407837\n",
      "Beta 2 2 Trained\n",
      "Epoch [10/30], Loss: 1.8643970489501953\n",
      "Epoch [20/30], Loss: 1.7037451267242432\n",
      "Epoch [30/30], Loss: 1.5897575616836548\n",
      "Beta 5 1 Trained\n",
      "Epoch [10/30], Loss: 1.6891162395477295\n",
      "Epoch [20/30], Loss: 1.605729579925537\n",
      "Epoch [30/30], Loss: 1.5584276914596558\n",
      "Beta 1 1 Trained\n",
      "Epoch [100/100], Loss: 1.3482465744018555\n",
      "Epoch [100/100], Loss: 1.457187294960022\n",
      "Epoch [100/100], Loss: 1.4524072408676147\n",
      "Epoch [100/100], Loss: 1.4492764472961426\n",
      "Epoch [100/100], Loss: 1.5084785223007202\n",
      "Epoch [100/100], Loss: 1.513051986694336\n",
      "Epoch [100/100], Loss: 1.553479790687561\n",
      "Epoch [100/100], Loss: 1.5764884948730469\n",
      "Epoch [100/100], Loss: 1.5762932300567627\n",
      "Epoch [100/100], Loss: 1.6216784715652466\n",
      "Epoch [100/100], Loss: 1.623788595199585\n",
      "Epoch [100/100], Loss: 1.62470281124115\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m results \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mstr\u001b[39m(true_alpha): {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGT\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miid\u001b[39m\u001b[38;5;124m\"\u001b[39m:[], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBeta_5_5\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBeta_5_1\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBeta_1_1\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPareto\u001b[39m\u001b[38;5;124m\"\u001b[39m:[]} \u001b[38;5;28;01mfor\u001b[39;00m true_alpha \u001b[38;5;129;01min\u001b[39;00m true_alphas}\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m num_seeds:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Reinitialize models with the new seed\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     ground_truths, iids,h_models \u001b[38;5;241m=\u001b[39m \u001b[43mreinitialize_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(true_alphas)):\n\u001b[1;32m     17\u001b[0m         ground_truth \u001b[38;5;241m=\u001b[39m execute_gt(true_alphas[index], ground_truths[index], data_dict_test)\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mreinitialize_models\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m      4\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfixed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dict_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dict_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_alphas\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mfixed\u001b[0;34m(data_dict_train, data_dict_test, dim, alphas)\u001b[0m\n\u001b[1;32m     26\u001b[0m groundtruths \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m alphas:\n\u001b[0;32m---> 28\u001b[0m     \u001b[43mARM_Regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrho\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dict_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     groundtruths\u001b[38;5;241m.\u001b[39mappend(copy\u001b[38;5;241m.\u001b[39mdeepcopy(f))\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m groundtruths, iids, [h_2_2, h_5_1, h_1_1, h_pareto]\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mARM_Regression.fit_f\u001b[0;34m(self, f, env_dict, alpha)\u001b[0m\n\u001b[1;32m     36\u001b[0m d \u001b[38;5;241m=\u001b[39m env_dict[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     37\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mMSELoss()        \n\u001b[0;32m---> 38\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m StepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/adam.py:87\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(weight_decay))\n\u001b[1;32m     84\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(lr\u001b[38;5;241m=\u001b[39mlr, betas\u001b[38;5;241m=\u001b[39mbetas, eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m     85\u001b[0m                 weight_decay\u001b[38;5;241m=\u001b[39mweight_decay, amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m     86\u001b[0m                 maximize\u001b[38;5;241m=\u001b[39mmaximize, foreach\u001b[38;5;241m=\u001b[39mforeach)\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mAdam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/optim/optimizer.py:49\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     47\u001b[0m param_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer got an empty parameter list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_groups[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     51\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n",
      "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "# New function to reset seeds and reinitialize models\n",
    "def reinitialize_models(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    return fixed(data_dict_train, data_dict_test, dim, true_alphas)\n",
    "\n",
    "# Main loop for iterating over different seeds\n",
    "num_seeds = [0,1,2]  # Number of different seeds to use\n",
    "true_alphas = [0.0, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99]\n",
    "results = {str(true_alpha): {\"GT\": [], \"iid\":[], \"Beta_5_5\": [], \"Beta_5_1\": [], \"Beta_1_1\": [], \"Pareto\":[]} for true_alpha in true_alphas}\n",
    "\n",
    "for seed in num_seeds:\n",
    "    # Reinitialize models with the new seed\n",
    "    ground_truths, iids,h_models = reinitialize_models(seed)\n",
    "    \n",
    "    for index in range(len(true_alphas)):\n",
    "        ground_truth = execute_gt(true_alphas[index], ground_truths[index], data_dict_test)\n",
    "        iid = execute_gt(true_alphas[index], iids[index], data_dict_test)\n",
    "        beta_5_5 = execute(true_alphas[index], h_models[0], data_dict_test)\n",
    "        beta_5_1 = execute(true_alphas[index], h_models[1], data_dict_test)\n",
    "        beta_1_1 = execute(true_alphas[index], h_models[2], data_dict_test)\n",
    "        pareto   = execute(true_alphas[index], h_models[3], data_dict_test)\n",
    "        # Collecting the results\n",
    "        results[str(true_alphas[index])][\"GT\"].append(ground_truth)\n",
    "        results[str(true_alphas[index])][\"iid\"].append(iid)\n",
    "        results[str(true_alphas[index])][\"Beta_5_5\"].append(beta_5_5)\n",
    "        results[str(true_alphas[index])][\"Beta_5_1\"].append(beta_5_1)\n",
    "        results[str(true_alphas[index])][\"Beta_1_1\"].append(beta_1_1)\n",
    "        results[str(true_alphas[index])][\"Pareto\"].append(pareto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa434fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistics(results):\n",
    "    means = {}\n",
    "    std_devs = {}\n",
    "\n",
    "    for key, value in results.items():\n",
    "        # Ensure value is a list of numbers, not a dictionary\n",
    "        if isinstance(value, dict):\n",
    "            means[key] = {sub_key: np.mean(sub_value) for sub_key, sub_value in value.items()}\n",
    "            std_devs[key] = {sub_key: np.std(sub_value) for sub_key, sub_value in value.items()}\n",
    "        else:\n",
    "            means[key] = np.mean(value)\n",
    "            std_devs[key] = np.std(value)\n",
    "\n",
    "    return means, std_devs\n",
    "means, std_devs = compute_statistics(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669623d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GT\")\n",
    "for key in results.keys():\n",
    "    print(round(means[key]['GT'],4),std_devs[key]['GT'])\n",
    "print(\"iid\")\n",
    "for key in results.keys():\n",
    "    print(round(means[key]['iid'],4),std_devs[key]['iid'])\n",
    "print(\"Beta_5_5\")\n",
    "for key in results.keys():\n",
    "    print(round(means[key]['Beta_5_5'],4),std_devs[key]['Beta_5_5'])\n",
    "print(\"Beta_5_1\")\n",
    "for key in results.keys():\n",
    "    print(round(means[key]['Beta_5_1'],4),std_devs[key]['Beta_5_1'])\n",
    "\n",
    "print(\"Beta_1_1\")\n",
    "for key in results.keys():\n",
    "    print(round(means[key]['Beta_1_1'],4),std_devs[key]['Beta_1_1'])\n",
    "    \n",
    "print(\"Pareto\")\n",
    "for key in results.keys():\n",
    "    print(round(means[key]['Pareto'],4),std_devs[key]['Pareto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03b2235",
   "metadata": {},
   "outputs": [],
   "source": [
    "data={}\n",
    "for learner in [\"GT\", \"iid\",\"Beta_5_5\", \"Beta_5_1\", \"Beta_1_1\", \"Pareto\"]:\n",
    "    data[learner]={\"mean\":[], \"std\":[]}\n",
    "    for key in results.keys():\n",
    "        data[learner][\"mean\"].append(means[key][learner])\n",
    "        data[learner][\"std\"].append(std_devs[key][learner]) \n",
    "    data[learner][\"mean\"].sort()\n",
    "    data[learner][\"std\"].sort()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fb832c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Different color for each learner\n",
    "import scienceplots\n",
    "plt.style.use(['science','no-latex'])\n",
    "from scipy.interpolate import make_interp_spline\n",
    "custom_legend_names = {\n",
    "    \"GT\": \"GT\",\n",
    "    \"iid\": \"$\\lambda_{User}=\\lambda_{Learner}$\",\n",
    "    \"Beta_5_5\":\"$a=5,b=5$\",\n",
    "    \"Beta_5_1\":\"$a=5,b=1$\",\n",
    "    \"Beta_1_1\":\"$a=1,b=1$\",\n",
    "    \"Pareto\": \"IRO\"\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    \"GT\": \"black\",\n",
    "    \"iid\": \"blue\", \n",
    "    \"Beta_5_5\": \"green\",\n",
    "    \"Beta_5_1\": \"orange\", \n",
    "    \"Beta_1_1\": \"red\",\n",
    "    \"Pareto\": \"Purple\"\n",
    "}\n",
    "lambdas = np.array(true_alphas)\n",
    "beta_params = {\n",
    "    \"Beta_5_5\": (5, 5),\n",
    "    \"Beta_5_1\": (5, 1),\n",
    "    \"Beta_1_1\": (1, 1)\n",
    "}\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# Plotting Beta distributions on the left plot\n",
    "for learner, (a, b) in beta_params.items():\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    y = beta.pdf(x, a, b)\n",
    "    ax1.plot(x, y, color=colors[learner], label=custom_legend_names[learner])\n",
    "ax1.plot([], [], color=colors[\"GT\"], label=custom_legend_names[\"GT\"])\n",
    "ax1.plot([], [], color=colors[\"Pareto\"], label=custom_legend_names[\"Pareto\"])\n",
    "ax1.set_title('$Beta(a,b)$ Distributions',fontsize=16)\n",
    "ax1.set_xlabel('$\\lambda_{Learner}$',fontsize=14)\n",
    "ax1.set_ylabel('Density',fontsize=14)\n",
    "ax1.grid(True)\n",
    "ax1.legend(fontsize=14)\n",
    "# Plotting data on the right plot\n",
    "for learner, color in colors.items():\n",
    "    if learner in data:\n",
    "        xnew = np.linspace(lambdas.min(), lambdas.max(), 300) \n",
    "        mean_spline = make_interp_spline(lambdas, data[learner][\"mean\"], k=3)\n",
    "        mean_smooth = mean_spline(xnew)\n",
    "        ax2.plot(xnew, mean_smooth, color=color, label=custom_legend_names[learner])\n",
    "\n",
    "        std_spline_up = make_interp_spline(lambdas, np.array(data[learner][\"mean\"]) + np.array(data[learner][\"std\"]), k=3)\n",
    "        std_spline_down = make_interp_spline(lambdas, np.array(data[learner][\"mean\"]) - np.array(data[learner][\"std\"]), k=3)\n",
    "        std_smooth_up, std_smooth_down = std_spline_up(xnew), std_spline_down(xnew)\n",
    "        ax2.fill_between(xnew, std_smooth_up, std_smooth_down, color=color, alpha=0.2)\n",
    "\n",
    "ax2.set_ylim(1.4, 2)\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_title('Ablation of $Q$ for IRO',fontsize=16)\n",
    "ax2.set_xlabel('$\\lambda_{User}$',fontsize=14)\n",
    "ax2.set_ylabel('$CVaR_\\lambda$',fontsize=14)\n",
    "ax2.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('beta_ablation_without_lambda_30.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b1cd4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
