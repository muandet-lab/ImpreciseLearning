{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3763af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.8/site-packages (0.12.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /opt/conda/lib/python3.8/site-packages (from seaborn) (3.5.2)\n",
      "Requirement already satisfied: pandas>=0.25 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.3.5)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.22.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.33.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.8)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.25->seaborn) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.1.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.1.4)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.9)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (8.3.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.29)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.5.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.12.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: asttokens in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: executing in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "owned-clearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "#%matplotlib notebook\n",
    "random_seed = 0\n",
    "np.random.seed(seed=random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "infinite-tourism",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.path.join(os.getcwd(), \"plot/\")\n",
    "today=datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77ec6888",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FHatNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(FHatNetwork, self).__init__()\n",
    "        layers = []\n",
    "        if hidden_sizes == []:\n",
    "            self.model = nn.Linear(input_size, output_size)\n",
    "        else:\n",
    "            layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "            layers.append(nn.ReLU())\n",
    "            for i in range(1, len(hidden_sizes)):\n",
    "                layers.append(nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]))\n",
    "                layers.append(nn.ReLU())\n",
    "            layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "            self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-spelling",
   "metadata": {},
   "source": [
    "**Preliminaries** In the following, the classes are defined to initiate the aggregation functions $\\rho_{\\alpha}$ and the aggregated risk minimization (ARM) optimization for a simple 1D and 2D regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "empirical-jungle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class aggregation_function:    \n",
    "    \"\"\" This class aggregates the risks. \"\"\"\n",
    "    def __init__(self, name:str):\n",
    "        self.name = name\n",
    "    def aggregate(self, risks, alpha) -> float:\n",
    "        if self.name == 'cvar':\n",
    "            return self.cvar(risks, alpha)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Currently, only CVaR is implemented.\")\n",
    "    def cvar(self, risks, alpha) -> float:\n",
    "        var = torch.quantile(risks,alpha, interpolation='linear')\n",
    "        cvar = risks[risks > var].mean()\n",
    "        return cvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "thick-surrey",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARM_Regression:\n",
    "    def __init__(self, name, experiment=\"1D_linear\"):      \n",
    "        self.aggregator = aggregation_function(name=name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def fit_LBFGS(self, f, env_dict, alpha):\n",
    "        \"\"\"Fit the coefficients of a function f using L-BFGS optimization.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        f : torch.nn.Module\n",
    "            The function to optimize.\n",
    "        env_dict : dict\n",
    "            A dictionary containing input features 'x' and labels 'y' for different environments.\n",
    "        alpha : int\n",
    "\n",
    "        Returns\n",
    "        ------\n",
    "        coef : float\n",
    "            Optimized coefficient.\n",
    "        \"\"\"\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            risks = torch.stack([loss_fn(env_dict[e]['y'].to(self.device), f(env_dict[e]['x'].to(self.device))) for e in env_dict.keys()])\n",
    "            cvar = self.aggregator.aggregate(risks, alpha)\n",
    "            cvar.backward()\n",
    "            return cvar\n",
    "        \n",
    "        d = env_dict[0]['x'].shape[1]\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        \n",
    "        # Set f to training mode and move it to the same device as the data\n",
    "        f.train()\n",
    "        f.to(self.device)\n",
    "        \n",
    "        optimizer = optim.LBFGS(f.parameters())\n",
    "        \n",
    "        max_iter = 2\n",
    "        for epoch in range(max_iter):\n",
    "            optimizer.step(closure)\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{max_iter}], Loss: {closure().item()}\")\n",
    "        \n",
    "        # Set f back to evaluation mode\n",
    "        f.eval()\n",
    "        \n",
    "        return f\n",
    "    \n",
    "    def fit(self, f, env_dict, alpha):        \n",
    "        \"\"\"Fit the coefficients of a function f. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env_dict : dict\n",
    "        alpha : int\n",
    "\n",
    "        Returns\n",
    "        ------\n",
    "        coef : float\n",
    "        \"\"\"\n",
    "        learning_rate = 0.01\n",
    "        num_epochs= 100\n",
    "        d = env_dict[0]['x'].shape[1]\n",
    "        loss_fn = torch.nn.MSELoss()        \n",
    "        optimizer = torch.optim.Adam(f.parameters(), lr=learning_rate)\n",
    "        scheduler = StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "        for epoch in range(num_epochs+1):\n",
    "            risks = torch.stack([loss_fn(env_dict[e]['y'].cuda(),f(env_dict[e]['x'].cuda())) for e in env_dict.keys()])\n",
    "            cvar = self.aggregator.aggregate(risks, alpha)\n",
    "            cvar.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch [{epoch}/{num_epochs}], Loss: {cvar.item()}\")\n",
    "        return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-genome",
   "metadata": {},
   "source": [
    "**Experiment 1** We assume the following *linear* data generation process $$Y(X) = X*\\theta_{e}+\\epsilon$$ and *nonlinear* data generation process $$Y(X) = sin(X*\\theta_{e})+\\epsilon$$.\n",
    "\n",
    "**Experiment 1A** Assume a linear model $Y_{e}=\\theta_{e}X+\\epsilon$, where $X \\sim \\mathcal{N}(2,0.2)$ and $\\epsilon\\sim \\mathcal{N}(0,0.1)$. We simulate different environments by drawing $\\theta$ from a beta distribution $Beta(0.1,0.2)$. In total, we generate for 25 environments 100 observations each.\n",
    "\n",
    "**Experiment 1B** Assume the same setting as in Experiment 1, however, in contrast, we simulate different environments by drawing $\\theta$ from a uniform distribution $\\ \\mathcal{U}(0,1)$. In total, we generate for 25 environments 100 observations each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pharmaceutical-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator:\n",
    "    \n",
    "    \"\"\" This class generates the simulation data. \"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self, envs_train = 25, envs_test = 25, \n",
    "                 size_train = 1000, size_test= 100, \n",
    "                 theta_dist=\"uniform\",\n",
    "                 dim=1):\n",
    "        \n",
    "        self.envs_train = envs_train\n",
    "        self.envs_test = envs_test\n",
    "        self.size_train = size_train\n",
    "        self.size_test = size_test\n",
    "        self.theta_dist = theta_dist\n",
    "        self.dim = dim \n",
    "        \n",
    "    def generate(self) -> dict:           \n",
    "        env_list_train = [f'e_{i}' for i in range(1,self.envs_train+1,1)]\n",
    "        env_dict_train = dict(list(enumerate(env_list_train)))\n",
    "        env_list_test  = [f'e_{i}' for i in range(1,self.envs_test+1,1)]\n",
    "        env_dict_test  = dict(list(enumerate(env_list_test)))\n",
    "        \n",
    "        \n",
    "        for e_train in env_dict_train.keys():\n",
    "            if self.theta_dist == \"uniform\": \n",
    "                distribution = dist.Uniform(0, 1)\n",
    "            else:\n",
    "                distribution = dist.Beta(0.1, 0.2)\n",
    "            theta_true = distribution.sample((self.dim,1))\n",
    "             \n",
    "            mean = torch.zeros(self.dim)  \n",
    "            covariance_matrix = torch.eye(self.dim)  \n",
    "            multivariate_normal = torch.distributions.MultivariateNormal(mean, covariance_matrix)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            x_train = multivariate_normal.sample(sample_shape=(self.size_train,))\n",
    "            noise_train = dist.normal.Normal(loc=0, scale=0.05).sample((self.size_train,1))\n",
    "            y_train = (1/math.sqrt(self.dim))*x_train@theta_true #+ noise_train\n",
    "            env_dict_train[e_train] = {'x': x_train,'y': y_train,'theta_true': theta_true}\n",
    "            \n",
    "        for e_test in env_dict_test.keys():\n",
    "            if self.theta_dist == \"uniform\": \n",
    "                distribution = dist.Uniform(0, 1)\n",
    "            else:\n",
    "                distribution = dist.Beta(0.1, 0.2)\n",
    "            theta_true = distribution.sample((self.dim,1))\n",
    "            mean = torch.zeros(self.dim)  \n",
    "            covariance_matrix = torch.eye(self.dim)  \n",
    "            multivariate_normal = torch.distributions.MultivariateNormal(mean, covariance_matrix)\n",
    "\n",
    "            x_test = multivariate_normal.sample(sample_shape=(self.size_test,))\n",
    "            noise_test = dist.normal.Normal(loc=0, scale=0.05).sample((self.size_test,1))\n",
    "            y_test = (1/math.sqrt(self.dim))*x_test@theta_true #+ noise_test\n",
    "            env_dict_test[e_test] = {'x': x_test,'y': y_test,'theta_true': theta_true}\n",
    "        return env_dict_train, env_dict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e61ea070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intitialize the experiment and generate the data\n",
    "envs_train, envs_test = 2500, 25\n",
    "size_train, size_test = 1000 , 1000\n",
    "theta_dist=\"beta\" \n",
    "rho=\"cvar\"\n",
    "dim = 1000\n",
    "generator = data_generator(envs_train, envs_test, size_train, size_test, theta_dist, dim)\n",
    "data_dict_train, data_dict_test = generator.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d847ff7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013080358505249023,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 15,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 20,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db111a4eacbb4686a85f8bf6df384b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Loss: 0.6188924908638\n",
      "Epoch [100/100], Loss: 0.1709335595369339\n",
      "alpha:  0.0  CVaR:  tensor(0.1709, device='cuda:0')\n",
      "alpha:  0.0  CVaR:  tensor(0.1696, device='cuda:0')\n",
      "Epoch [0/100], Loss: 0.17188389599323273\n",
      "Epoch [100/100], Loss: 0.17187586426734924\n",
      "alpha:  0.05  CVaR:  tensor(0.1719, device='cuda:0')\n",
      "alpha:  0.05  CVaR:  tensor(0.1700, device='cuda:0')\n",
      "Epoch [0/100], Loss: 0.17265886068344116\n",
      "Epoch [100/100], Loss: 0.17265251278877258\n",
      "alpha:  0.1  CVaR:  tensor(0.1727, device='cuda:0')\n",
      "alpha:  0.1  CVaR:  tensor(0.1705, device='cuda:0')\n",
      "Epoch [0/100], Loss: 0.17337161302566528\n",
      "Epoch [100/100], Loss: 0.17336517572402954\n",
      "alpha:  0.15  CVaR:  tensor(0.1734, device='cuda:0')\n",
      "alpha:  0.15  CVaR:  tensor(0.1710, device='cuda:0')\n",
      "Epoch [0/100], Loss: 0.1740560680627823\n",
      "Epoch [100/100], Loss: 0.17404820024967194\n",
      "alpha:  0.2  CVaR:  tensor(0.1740, device='cuda:0')\n",
      "alpha:  0.2  CVaR:  tensor(0.1715, device='cuda:0')\n",
      "Epoch [0/100], Loss: 0.17471162974834442\n",
      "Epoch [100/100], Loss: 0.17470404505729675\n",
      "alpha:  0.25  CVaR:  tensor(0.1747, device='cuda:0')\n",
      "alpha:  0.25  CVaR:  tensor(0.1725, device='cuda:0')\n",
      "Epoch [0/100], Loss: 0.1753559410572052\n",
      "Epoch [100/100], Loss: 0.17534808814525604\n",
      "alpha:  0.3  CVaR:  tensor(0.1753, device='cuda:0')\n",
      "alpha:  0.3  CVaR:  tensor(0.1731, device='cuda:0')\n",
      "Epoch [0/100], Loss: 0.17600499093532562\n",
      "Epoch [100/100], Loss: 0.1759967803955078\n",
      "alpha:  0.35  CVaR:  tensor(0.1760, device='cuda:0')\n",
      "alpha:  0.35  CVaR:  tensor(0.1736, device='cuda:0')\n",
      "Epoch [0/100], Loss: 0.17665734887123108\n",
      "Epoch [100/100], Loss: 0.1766471266746521\n",
      "alpha:  0.4  CVaR:  tensor(0.1766, device='cuda:0')\n",
      "alpha:  0.4  CVaR:  tensor(0.1742, device='cuda:0')\n",
      "Epoch [0/100], Loss: 0.17731867730617523\n",
      "Epoch [100/100], Loss: 0.17730778455734253\n",
      "alpha:  0.45  CVaR:  tensor(0.1773, device='cuda:0')\n",
      "alpha:  0.45  CVaR:  tensor(0.1748, device='cuda:0')\n",
      "Epoch [0/100], Loss: 0.1780005395412445\n",
      "Epoch [100/100], Loss: 0.1779870241880417\n",
      "alpha:  0.5  CVaR:  tensor(0.1780, device='cuda:0')\n",
      "alpha:  0.5  CVaR:  tensor(0.1760, device='cuda:0')\n",
      "Epoch [0/100], Loss: 0.17870944738388062\n",
      "Epoch [100/100], Loss: 0.17869460582733154\n",
      "alpha:  0.55  CVaR:  tensor(0.1787, device='cuda:0')\n",
      "alpha:  0.55  CVaR:  tensor(0.1765, device='cuda:0')\n",
      "Epoch [0/100], Loss: 0.1794653981924057\n",
      "Epoch [100/100], Loss: 0.17944875359535217\n",
      "alpha:  0.6  CVaR:  tensor(0.1794, device='cuda:0')\n",
      "alpha:  0.6  CVaR:  tensor(0.1769, device='cuda:0')\n",
      "Epoch [0/100], Loss: 0.18028219044208527\n"
     ]
    }
   ],
   "source": [
    "#Run the ARM regression task\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "results = pd.DataFrame(columns = [\"alpha\", \"theta_hat\", \"cvar\"])\n",
    "fig, ax = plt.subplots(1, 2, sharey=True, figsize=(16,6))\n",
    "rg = range(0,100,5)\n",
    "diverging_colors = sns.color_palette(\"RdBu\", len(rg))\n",
    "j=1\n",
    "f = FHatNetwork(dim,[],1).cuda()\n",
    "for i in tqdm(rg):\n",
    "    ARM_Regression(name=rho).fit(f, data_dict_train, i/100)  \n",
    "    with torch.no_grad():\n",
    "        risks = torch.stack([loss_fn(data_dict_train[e]['y'].cuda(), f(data_dict_train[e]['x'].cuda())) for e in data_dict_train.keys()])\n",
    "        cvar_emp = aggregation_function(name=\"cvar\").aggregate(risks, i/100)     \n",
    "        sns.kdeplot(risks.cpu().numpy(), ax=ax[0],color=diverging_colors[len(rg)-j], label=str(i/100))\n",
    "        print(\"alpha: \", i/100, \" CVaR: \", cvar_emp)#, \"theta: \", f.model.weight.data)\n",
    "        risks = torch.stack([loss_fn(data_dict_test[e]['y'].cuda(), f(data_dict_test[e]['x'].cuda())) for e in data_dict_test.keys()])\n",
    "        cvar_emp = aggregation_function(name=\"cvar\").aggregate(risks, i/100)     \n",
    "        sns.kdeplot(risks.cpu().numpy(), ax=ax[1],color=diverging_colors[len(rg)-j], label=str(i/100))\n",
    "        print(\"alpha: \", i/100, \" CVaR: \", cvar_emp)\n",
    "    results.at[i, \"alpha\"] = i/100\n",
    "    results.at[i, \"theta_hat\"] = f.state_dict()\n",
    "    results.at[i, \"cvar\"] = cvar_emp\n",
    "    j+=1\n",
    "\n",
    "ax[0].legend(bbox_to_anchor=(1.02, 1.02), title=r\"$\\alpha$\", loc='upper left')\n",
    "ax[1].legend(bbox_to_anchor=(1.02, 1.02), title=r\"$\\alpha$\", loc='upper left')\n",
    "\n",
    "ax[0].set_title(r\"Dist of risks on train for independent optimization of $f_\\theta$\")\n",
    "ax[1].set_title(r\"Dist of risks on val for independent optimization of $f_\\theta$\")\n",
    "norm = plt.Normalize(0.05, 0.95)\n",
    "sm = plt.cm.ScalarMappable(cmap=\"RdBu_r\", norm=norm)\n",
    "sm.set_array([])\n",
    "ax[0].get_legend().remove()\n",
    "ax[1].get_legend().remove()\n",
    "# Add colorbar\n",
    "ax[1].figure.colorbar(sm).set_label(label=r'$\\alpha$', labelpad=5, size=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9872b6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
