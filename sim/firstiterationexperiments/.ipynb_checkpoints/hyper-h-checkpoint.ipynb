{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8438efca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.8/site-packages (0.13.0)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.3.5)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.22.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.3 in /opt/conda/lib/python3.8/site-packages (from seaborn) (3.5.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (9.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (1.4.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (4.33.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (3.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.3->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.3->seaborn) (1.16.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.1.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.9)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (8.3.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.29)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.5.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.12.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: asttokens in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: executing in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn\n",
    "!pip install ipywidgets\n",
    "#!pip install hypnettorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-clearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import scipy.stats\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "#%matplotlib notebook\n",
    "random_seed = 1\n",
    "np.random.seed(seed=1)\n",
    "torch.manual_seed(2)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-tourism",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.path.join(os.getcwd(), \"plot/\")\n",
    "today=datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8308a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ec6888",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FHatNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(FHatNetwork, self).__init__()\n",
    "        layers = []\n",
    "        if hidden_sizes == []:\n",
    "            self.model = nn.Linear(input_size, output_size)\n",
    "        else:\n",
    "            layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "            layers.append(nn.ReLU())\n",
    "            for i in range(1, len(hidden_sizes)):\n",
    "                layers.append(nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]))\n",
    "                layers.append(nn.ReLU())\n",
    "            layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "            self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the target network\n",
    "class TargetNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(TargetNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Define the hypernetwork\n",
    "class HyperNetwork(nn.Module):\n",
    "    def __init__(self, target_net, hyper_input_dim, hyper_hidden_dims):\n",
    "        super(HyperNetwork, self).__init__()\n",
    "        self.target_net = target_net\n",
    "        layers = [] \n",
    "        if hyper_hidden_dims == []:\n",
    "            layers.append(nn.Linear(hyper_input_dim, target_net.fc.weight.numel()))\n",
    "        else:\n",
    "            layers.append(nn.Linear(hyper_input_dim, hyper_hidden_dims[0]))\n",
    "            layers.append(nn.ReLU())\n",
    "            for i in range(1, len(hyper_hidden_dims)):\n",
    "                layers.append(nn.Linear(hyper_hidden_dims[i-1],hyper_hidden_dims[i]))\n",
    "            layers.append(nn.Linear(hyper_hidden_dims[-1], target_net.fc.weight.numel()))\n",
    "        \n",
    "        layers.append(nn.Tanh())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x, alpha):\n",
    "        alpha = alpha.unsqueeze(0)\n",
    "        params = self.model(alpha)\n",
    "        params = params.view(self.target_net.fc.weight.size())\n",
    "        self.target_net.fc.weight.data = params\n",
    "        return self.target_net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-spelling",
   "metadata": {},
   "source": [
    "**Preliminaries** In the following, the classes are defined to initiate the aggregation functions $\\rho_{\\alpha}$ and the aggregated risk minimization (ARM) optimization for a simple 1D and 2D regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-jungle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class aggregation_function:    \n",
    "    \"\"\" This class aggregates the risks. \"\"\"\n",
    "    def __init__(self, name:str):\n",
    "        self.name = name\n",
    "    def aggregate(self, risks, alpha) -> float:\n",
    "        if self.name == 'cvar':\n",
    "            return self.cvar(risks, alpha)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Currently, only CVaR is implemented.\")\n",
    "    def cvar(self, risks, alpha) -> float:\n",
    "        var = torch.quantile(risks,alpha, interpolation='linear')\n",
    "        cvar = risks[risks > var].mean()\n",
    "        return cvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-surrey",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARM_Regression:\n",
    "    def __init__(self, name, experiment=\"1D_linear\"):      \n",
    "        self.aggregator = aggregation_function(name=name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    def fit_h(self, h, env_dict):\n",
    "        \"\"\"Fit the coefficients of a function h in optimization. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env_dict : dict\n",
    "        alpha : int\n",
    "\n",
    "        Returns\n",
    "        ------\n",
    "        coef : float\n",
    "        \"\"\"\n",
    "        learning_rate = 0.01\n",
    "        num_epochs=200\n",
    "        loss_fn = torch.nn.MSELoss()        \n",
    "        optimizer = torch.optim.Adam(h.parameters(), lr=learning_rate)\n",
    "        scheduler = StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "        alpha_primes = torch.arange(0,1,0.05).cuda()\n",
    "        for alpha_prime in alpha_primes:        \n",
    "            print(f\"alpha: {alpha_prime}\")\n",
    "            for epoch in range(num_epochs+1):\n",
    "                risks = torch.stack([loss_fn(env_dict[e]['y'].cuda(), h(env_dict[e]['x'].cuda(), alpha_prime)) for e in env_dict.keys()])\n",
    "                loss = self.aggregator.aggregate(risks, alpha_prime)\n",
    "                if epoch % 100 == 0:\n",
    "                    print(f\"Epoch [{epoch}/{num_epochs}], Loss: {loss.item()}, param:{h.target_net.fc.weight.data.item()}\")\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "        return\n",
    "    \n",
    "    def fit_f(self, f, env_dict, alpha):        \n",
    "        \"\"\"Fit the coefficients of a function f. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env_dict : dict\n",
    "        alpha : int\n",
    "\n",
    "        Returns\n",
    "        ------\n",
    "        coef : float\n",
    "        \"\"\"\n",
    "        learning_rate = 0.01\n",
    "        num_epochs= 200\n",
    "        d = env_dict[0]['x'].shape[1]\n",
    "        loss_fn = torch.nn.MSELoss()        \n",
    "        optimizer = torch.optim.Adam(f.parameters(), lr=learning_rate)\n",
    "        scheduler = StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "        for epoch in range(num_epochs):\n",
    "            risks = torch.stack([loss_fn(env_dict[e]['y'].cuda(),f(env_dict[e]['x'].cuda())) for e in env_dict.keys()])\n",
    "            cvar = self.aggregator.aggregate(risks, alpha)\n",
    "            cvar.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {cvar.item()}\")\n",
    "        return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-genome",
   "metadata": {},
   "source": [
    "**Experiment 1** We assume the following *linear* data generation process $$Y(X) = X*\\theta_{e}+\\epsilon$$ and *nonlinear* data generation process $$Y(X) = sin(X*\\theta_{e})+\\epsilon$$.\n",
    "\n",
    "**Experiment 1A** Assume a linear model $Y_{e}=\\theta_{e}X+\\epsilon$, where $X \\sim \\mathcal{N}(2,0.2)$ and $\\epsilon\\sim \\mathcal{N}(0,0.1)$. We simulate different environments by drawing $\\theta$ from a beta distribution $Beta(0.1,0.2)$. In total, we generate for 25 environments 100 observations each.\n",
    "\n",
    "**Experiment 1B** Assume the same setting as in Experiment 1, however, in contrast, we simulate different environments by drawing $\\theta$ from a uniform distribution $\\ \\mathcal{U}(0,1)$. In total, we generate for 25 environments 100 observations each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator:\n",
    "    \n",
    "    \"\"\" This class generates the simulation data. \"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self, envs_train = 25, envs_test = 25, \n",
    "                 size_train = 1000, size_test= 100, \n",
    "                 theta_dist=\"uniform\",\n",
    "                 dim=1):\n",
    "        \n",
    "        self.envs_train = envs_train\n",
    "        self.envs_test = envs_test\n",
    "        self.size_train = size_train\n",
    "        self.size_test = size_test\n",
    "        self.theta_dist = theta_dist\n",
    "        self.dim = dim \n",
    "        \n",
    "    def generate(self) -> dict:           \n",
    "        env_list_train = [f'e_{i}' for i in range(1,self.envs_train+1,1)]\n",
    "        env_dict_train = dict(list(enumerate(env_list_train)))\n",
    "        env_list_test  = [f'e_{i}' for i in range(1,self.envs_test+1,1)]\n",
    "        env_dict_test  = dict(list(enumerate(env_list_test)))\n",
    "        \n",
    "        \n",
    "        for e_train in env_dict_train.keys():\n",
    "            if self.theta_dist == \"uniform\": \n",
    "                distribution = dist.Uniform(0, 1)\n",
    "            else:\n",
    "                distribution = dist.Beta(0.1, 0.2)\n",
    "            theta_true = distribution.sample((self.dim,1))\n",
    "            x_train = dist.normal.Normal(loc=1.0/self.dim, scale=0.5).sample((self.size_train,self.dim))\n",
    "            noise_train = dist.normal.Normal(loc=0, scale=0.05).sample((self.size_train,self.dim))\n",
    "            y_train = (1.0/math.sqrt(self.dim))*x_train@theta_true + noise_train\n",
    "            env_dict_train[e_train] = {'x': x_train,'y': y_train,'theta_true': theta_true}\n",
    "            \n",
    "        for e_test in env_dict_test.keys():\n",
    "            if self.theta_dist == \"uniform\": \n",
    "                distribution = dist.Uniform(0, 1)\n",
    "            else:\n",
    "                distribution = dist.Beta(0.1, 0.2)\n",
    "            theta_true = distribution.sample((self.dim,1))\n",
    "            x_test = dist.normal.Normal(loc=1.0/self.dim, scale=0.5).sample((self.size_test,self.dim))\n",
    "            noise_test = dist.normal.Normal(loc=0, scale=0.05).sample((self.size_test,self.dim))\n",
    "            y_test = (1.0/math.sqrt(self.dim))*x_test@theta_true + noise_test\n",
    "            env_dict_test[e_test] = {'x': x_test,'y': y_test,'theta_true': theta_true}\n",
    "            \n",
    "        return env_dict_train, env_dict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-instrument",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Intitialize the experiment and generate the data\n",
    "envs_train, envs_test = 250, 25\n",
    "size_train, size_test = 100 , 100\n",
    "theta_dist=\"beta\" \n",
    "rho=\"cvar\"\n",
    "dim = 1\n",
    "generator = data_generator(envs_train, envs_test, size_train, size_test, theta_dist, dim)\n",
    "data_dict_train, data_dict_test = generator.generate()\n",
    "# Define some example dimensions\n",
    "generator = data_generator(envs_train, envs_test, size_train, size_test, theta_dist, dim)\n",
    "data_dict_train, data_dict_test = generator.generate()\n",
    "#fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
    "\n",
    "X_train = [data_dict_train[e]['x'].ravel() for e in data_dict_train.keys()]\n",
    "X_train = np.array([item for sublist in X_train for item in sublist]).reshape(-1,1)\n",
    "Y_train = [data_dict_train[e]['y'] for e in data_dict_train.keys()]\n",
    "Y_train = np.array([item for sublist in Y_train for item in sublist]).reshape(-1,1)\n",
    "\n",
    "#ax.scatter(y=Y_train, x=X_train, s=3, c=\"grey\", alpha=0.05, label=\"Training envs\")\n",
    "#ax.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-refund",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Instantiate the target network and hypernetwork\n",
    "target_net = TargetNetwork(input_dim=dim, output_dim=1)\n",
    "h = HyperNetwork(target_net, hyper_input_dim=1, hyper_hidden_dims=[10]).cuda()\n",
    "#Run the ARM regression task\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "results = pd.DataFrame(columns = [\"alpha\", \"cvar\"])\n",
    "fig, ax = plt.subplots(1, 1, sharey=True, figsize=(8,6))\n",
    "rg = torch.arange(0,1,0.05)\n",
    "diverging_colors = sns.color_palette(\"RdBu\", len(rg))\n",
    "ARM_Regression(name=rho).fit_h(h, data_dict_train) \n",
    "h.eval()\n",
    "j=1\n",
    "for alpha in tqdm(rg):\n",
    "    with torch.no_grad():\n",
    "        risks = torch.stack([loss_fn(data_dict_train[e]['y'].cuda(), h(data_dict_train[e]['x'].cuda(), alpha.cuda())) for e in data_dict_train.keys()])\n",
    "        cvar_emp = aggregation_function(name=\"cvar\").aggregate(risks.cpu(), alpha.cpu())     \n",
    "        sns.kdeplot(risks.cpu().numpy(), ax=ax,color=diverging_colors[len(rg)-j], label=str(alpha.item()))\n",
    "        print(\"alpha: \", alpha, \" CVaR: \",cvar_emp.item(), \"param: \",h.target_net.fc.weight.data.item() )\n",
    "    results.at[int(alpha.item()*100), \"alpha\"] = alpha\n",
    "    results.at[int(alpha.item()*100), \"cvar\"] = cvar_emp\n",
    "    j+=1\n",
    "ax.legend(bbox_to_anchor=(1.02, 1.02), title=r\"$\\alpha$\", loc='upper left')\n",
    "ax.set_title(r\"Dist of risks on train for optimization of $h_\\beta(x,\\alpha)$\")\n",
    "norm = plt.Normalize(0.05, 0.95)\n",
    "sm = plt.cm.ScalarMappable(cmap=\"RdBu_r\", norm=norm)\n",
    "sm.set_array([])\n",
    "ax.get_legend().remove()\n",
    "ax.figure.colorbar(sm).set_label(label= r'$\\alpha$',labelpad=5,size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584bbaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the ARM regression task\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "results = pd.DataFrame(columns = [\"alpha\", \"theta_hat\", \"cvar\"])\n",
    "fig, ax = plt.subplots(1, 2, sharey=True, figsize=(16,6))\n",
    "rg = range(0,100,5)\n",
    "diverging_colors = sns.color_palette(\"RdBu\", len(rg))\n",
    "j=1\n",
    "f = FHatNetwork(dim,[],1).cuda()\n",
    "for i in tqdm(rg):\n",
    "    ARM_Regression(name=rho).fit_f(f, data_dict_train, i/100)  \n",
    "    print(\"alpha: \", i/100, f.model.weight.data.item())\n",
    "    with torch.no_grad():\n",
    "        risks = torch.stack([loss_fn(data_dict_train[e]['y'].cuda(), f(data_dict_train[e]['x'].cuda())) for e in data_dict_train.keys()])\n",
    "        cvar_emp = aggregation_function(name=\"cvar\").aggregate(risks, i/100)     \n",
    "        sns.kdeplot(risks.cpu().numpy(), ax=ax[0],color=diverging_colors[len(rg)-j], label=str(i/100))\n",
    "        print(\" CVaR: \",cvar_emp)\n",
    "        #test \n",
    "        risks = torch.stack([loss_fn(data_dict_test[e]['y'].cuda(), f(data_dict_test[e]['x'].cuda())) for e in data_dict_test.keys()])\n",
    "        cvar_emp = aggregation_function(name=\"cvar\").aggregate(risks, i/100)     \n",
    "        sns.kdeplot(risks.cpu().numpy(), ax=ax[1],color=diverging_colors[len(rg)-j], label=str(i/100))\n",
    "    \n",
    "    results.at[i, \"alpha\"] = i/100\n",
    "    results.at[i, \"theta_hat\"] = f.state_dict()\n",
    "    results.at[i, \"cvar\"] = cvar_emp\n",
    "    j+=1\n",
    "\n",
    "ax[0].legend(bbox_to_anchor=(1.02, 1.02), title=r\"$\\alpha$\", loc='upper left')\n",
    "ax[1].legend(bbox_to_anchor=(1.02, 1.02), title=r\"$\\alpha$\", loc='upper left')\n",
    "\n",
    "ax[0].set_title(r\"Dist of risks on train for independent optimization of $f_\\theta$\")\n",
    "ax[1].set_title(r\"Dist of risks on val for independent optimization of $f_\\theta$\")\n",
    "norm = plt.Normalize(0.05, 0.95)\n",
    "sm = plt.cm.ScalarMappable(cmap=\"RdBu_r\", norm=norm)\n",
    "sm.set_array([])\n",
    "ax[0].get_legend().remove()\n",
    "ax[1].get_legend().remove()\n",
    "# Add colorbar\n",
    "ax[1].figure.colorbar(sm).set_label(label=r'$\\alpha$', labelpad=5, size=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc00b3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
