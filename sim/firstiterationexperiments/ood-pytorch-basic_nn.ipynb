{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ddbb015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "\u001b[K     |████████████████████████████████| 293 kB 5.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.25 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.3.5)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.22.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /opt/conda/lib/python3.8/site-packages (from seaborn) (3.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.8)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.33.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.0.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.25->seaborn) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.12.2\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.0-py3-none-any.whl (139 kB)\n",
      "\u001b[K     |████████████████████████████████| 139 kB 8.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting comm>=0.1.3\n",
      "  Downloading comm-0.1.4-py3-none-any.whl (6.6 kB)\n",
      "Collecting widgetsnbextension~=4.0.7\n",
      "  Downloading widgetsnbextension-4.0.8-py3-none-any.whl (2.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3 MB 56.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (8.3.0)\n",
      "Collecting jupyterlab-widgets~=3.0.7\n",
      "  Downloading jupyterlab_widgets-3.0.8-py3-none-any.whl (214 kB)\n",
      "\u001b[K     |████████████████████████████████| 214 kB 99.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.5.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.29)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.12.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: executing in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, comm, ipywidgets\n",
      "Successfully installed comm-0.1.4 ipywidgets-8.1.0 jupyterlab-widgets-3.0.8 widgetsnbextension-4.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "owned-clearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "#%matplotlib notebook\n",
    "random_seed = 1\n",
    "np.random.seed(seed=random_seed)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "infinite-tourism",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.path.join(os.getcwd(), \"plot/\")\n",
    "today=datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77ec6888",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FHatNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(FHatNetwork, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            layers.append(nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51fbf9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaEmbeddingNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(AlphaEmbeddingNetwork, self).__init__()\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, hidden_size)\n",
    "   \n",
    "    def forward(self, alpha):\n",
    "        # Convert the scalar alpha to a vector\n",
    "        print(\"alpha\", alpha, alpha.shape)\n",
    "        alpha_vector = torch.relu(torch.unsqueeze(alpha, 1))\n",
    "        hidden1 = self.relu(self.linear1(alpha_vector))\n",
    "        hidden2 = self.relu(self.linear2(hidden1))\n",
    "        theta = self.relu(self.linear3(hidden2))\n",
    "        return theta\n",
    "\n",
    "\n",
    "class HNetwork(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(HNetwork, self).__init__()\n",
    "        self.embedding_network = AlphaEmbeddingNetwork(input_size=1, hidden_size=embedding_size)\n",
    "        self.f_network = FHatNetwork(embedding_size)\n",
    "        \n",
    "    def forward(self, x, alpha):\n",
    "        # Get the embedding theta from the embedding network\n",
    "        alpha = torch.tensor([alpha])\n",
    "        theta = self.embedding_network(alpha)\n",
    "        self.f_network.linear.weight = theta\n",
    "        output = self.f_network(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-spelling",
   "metadata": {},
   "source": [
    "**Preliminaries** In the following, the classes are defined to initiate the aggregation functions $\\rho_{\\alpha}$ and the aggregated risk minimization (ARM) optimization for a simple 1D and 2D regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "empirical-jungle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class aggregation_function:    \n",
    "    \"\"\" This class aggregates the risks. \"\"\"\n",
    "    def __init__(self, name:str):\n",
    "        self.name = name\n",
    "    def aggregate(self, risks, alpha) -> float:\n",
    "        if self.name == 'cvar':\n",
    "            return self.cvar(risks, alpha)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Currently, only CVaR is implemented.\")\n",
    "    def cvar(self, risks, alpha) -> float:\n",
    "        var = torch.quantile(risks,alpha, interpolation='linear')\n",
    "        cvar = risks[risks > var].mean()\n",
    "        return cvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "thick-surrey",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARM_Regression:\n",
    "    def __init__(self,name,experiment=\"1D_linear\"):      \n",
    "        self.aggregator = aggregation_function(name=name)\n",
    "        \n",
    "    \n",
    "    def fit(self, f, env_dict, alpha):        \n",
    "        \"\"\"Fit the coefficients of a function f. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env_dict : dict\n",
    "        alpha : int\n",
    "\n",
    "        Returns\n",
    "        ------\n",
    "        coef : float\n",
    "        \"\"\"\n",
    "        learning_rate = 0.01\n",
    "        num_epochs= 200\n",
    "        d = env_dict[0]['x'].shape[1]\n",
    "        loss_fn = torch.nn.MSELoss()        \n",
    "        optimizer = torch.optim.Adam(f.parameters(), lr=learning_rate)\n",
    "        scheduler = StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "        for epoch in range(num_epochs):\n",
    "            risks = torch.stack([loss_fn(env_dict[e]['y'],f(env_dict[e]['x'])) for e in env_dict.keys()])\n",
    "            cvar = self.aggregator.aggregate(risks, alpha)\n",
    "            cvar.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {cvar.item()}\")\n",
    "        return \n",
    "    \n",
    "    def fit_independent_h_loop(self, h, env_dict):\n",
    "        \"\"\"Fit the coefficients of a function h in independent optimization. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env_dict : dict\n",
    "        alpha : int\n",
    "\n",
    "        Returns\n",
    "        ------\n",
    "        coef : float\n",
    "        \"\"\"\n",
    "        learning_rate = 0.01\n",
    "        num_epochs=200\n",
    "        loss_fn = torch.nn.MSELoss()        \n",
    "        optimizer = torch.optim.Adam(h.parameters(), lr=learning_rate)\n",
    "        scheduler = StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "        for alpha in torch.arange(0,1,0.05):\n",
    "            for epoch in range(num_epochs): \n",
    "                risks = torch.stack([loss_fn(env_dict[e]['y'],h(env_dict[e]['x'], alpha)) for e in env_dict.keys()])\n",
    "                cvar = self.aggregator.aggregate(risks, alpha)\n",
    "                optimizer.zero_grad()\n",
    "                cvar.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                #if (epoch + 1) % 10 == 0:\n",
    "                #    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {cvar.item()}\")\n",
    "        return \n",
    "    \n",
    "    def fit_independent_h_probablistic(self, h, env_dict):\n",
    "        \"\"\"Fit the coefficients of a function h in independent optimization  with alpha'~U[0.05,1]. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env_dict : dict\n",
    "        alpha : int\n",
    "\n",
    "        Returns\n",
    "        ------\n",
    "        coef : float\n",
    "        \"\"\"\n",
    "        learning_rate = 0.01\n",
    "        num_epochs=200\n",
    "        loss_fn = torch.nn.MSELoss()        \n",
    "        optimizer = torch.optim.Adam(h.parameters(), lr=learning_rate)\n",
    "        scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "        for epoch in range(num_epochs): \n",
    "            alpha = dist.Uniform(0,1).sample()\n",
    "            risks = torch.stack([loss_fn(env_dict[e]['y'],h(env_dict[e]['x'], alpha)) for e in env_dict.keys()])\n",
    "            cvar = self.aggregator.aggregate(risks, alpha)\n",
    "            cvar.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            #if (epoch + 1) % 10 == 0:\n",
    "            #    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {cvar.item()}\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def objective_h(self, risks, alpha_prime):\n",
    "        def pdf_normal_distribution(x, alpha_prime):\n",
    "            # Create a normal distribution centered at alpha' with std. 1 at alpha'\n",
    "            likelihood = dist.normal.Normal(loc=alpha_prime, scale=alpha_prime)\n",
    "            # Calculate the probability density at the point x\n",
    "            pdf = likelihood.log_prob(x).exp()\n",
    "            return pdf\n",
    "        alphas = dist.Uniform(0,1).sample((1000,1))\n",
    "        objective = 0\n",
    "        weights = pdf_normal_distribution(alphas, alpha_prime)\n",
    "        cvars = torch.stack([self.aggregator.aggregate(risks,alpha) for alpha in alphas])\n",
    "        objective = (weights * cvars).mean()\n",
    "        return objective\n",
    "    \n",
    "    def fit_h(self, h, env_dict):\n",
    "        \"\"\"Fit the coefficients of a function h in optimization. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env_dict : dict\n",
    "        alpha : int\n",
    "\n",
    "        Returns\n",
    "        ------\n",
    "        coef : float\n",
    "        \"\"\"\n",
    "        learning_rate = 0.01\n",
    "        num_epochs=200\n",
    "        loss_fn = torch.nn.MSELoss()        \n",
    "        optimizer = torch.optim.Adam(h.parameters(), lr=learning_rate)\n",
    "        scheduler = StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "        for epoch in range(num_epochs): \n",
    "            alpha_prime = dist.Uniform(0.05,1).sample()\n",
    "            risks = torch.stack([loss_fn(env_dict[e]['y'],h(env_dict[e]['x'], alpha_prime)) for e in env_dict.keys()])\n",
    "            loss = self.objective_h(risks, alpha_prime)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            #if (epoch + 1) % 10 == 0:\n",
    "            #    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {cvar.item()}\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-genome",
   "metadata": {},
   "source": [
    "**Experiment 1** We assume the following *linear* data generation process $$Y(X) = X*\\theta_{e}+\\epsilon$$ and *nonlinear* data generation process $$Y(X) = sin(X*\\theta_{e})+\\epsilon$$.\n",
    "\n",
    "**Experiment 1A** Assume a linear model $Y_{e}=\\theta_{e}X+\\epsilon$, where $X \\sim \\mathcal{N}(2,0.2)$ and $\\epsilon\\sim \\mathcal{N}(0,0.1)$. We simulate different environments by drawing $\\theta$ from a beta distribution $Beta(0.1,0.2)$. In total, we generate for 25 environments 100 observations each.\n",
    "\n",
    "**Experiment 1B** Assume the same setting as in Experiment 1, however, in contrast, we simulate different environments by drawing $\\theta$ from a uniform distribution $\\ \\mathcal{U}(0,1)$. In total, we generate for 25 environments 100 observations each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pharmaceutical-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generator:\n",
    "    \n",
    "    \"\"\" This class generates the simulation data. \"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self, envs_train = 25, envs_test = 25, \n",
    "                 size_train = 1000, size_test= 100, \n",
    "                 theta_dist=\"uniform\",\n",
    "                 dim=1):\n",
    "        \n",
    "        self.envs_train = envs_train\n",
    "        self.envs_test = envs_test\n",
    "        self.size_train = size_train\n",
    "        self.size_test = size_test\n",
    "        self.theta_dist = theta_dist\n",
    "        self.dim = dim \n",
    "        \n",
    "    def generate(self) -> dict:           \n",
    "        env_list_train = [f'e_{i}' for i in range(1,self.envs_train+1,1)]\n",
    "        env_dict_train = dict(list(enumerate(env_list_train)))\n",
    "        env_list_test  = [f'e_{i}' for i in range(1,self.envs_test+1,1)]\n",
    "        env_dict_test  = dict(list(enumerate(env_list_test)))\n",
    "        \n",
    "        \n",
    "        for e_train in env_dict_train.keys():\n",
    "            if self.theta_dist == \"uniform\": \n",
    "                distribution = dist.Uniform(0, 1)\n",
    "            else:\n",
    "                distribution = dist.Beta(0.1, 0.2)\n",
    "            theta_true = distribution.sample((self.dim,1))\n",
    "            x_train = dist.normal.Normal(loc=1, scale=0.5).sample((self.size_train,self.dim))\n",
    "            noise_train = dist.normal.Normal(loc=0, scale=0.05).sample((self.size_train,self.dim))\n",
    "            y_train = x_train@theta_true + noise_train\n",
    "            env_dict_train[e_train] = {'x': x_train,'y': y_train,'theta_true': theta_true}\n",
    "            \n",
    "        for e_test in env_dict_test.keys():\n",
    "            if self.theta_dist == \"uniform\": \n",
    "                distribution = dist.Uniform(0, 1)\n",
    "            else:\n",
    "                distribution = dist.Beta(0.1, 0.2)\n",
    "            theta_true = distribution.sample((self.dim,1))\n",
    "            x_test = dist.normal.Normal(loc=1, scale=0.5).sample((self.size_test,self.dim))\n",
    "            noise_test = dist.normal.Normal(loc=0, scale=0.05).sample((self.size_test,self.dim))\n",
    "            y_test = x_test@theta_true+noise_test\n",
    "            env_dict_test[e_test] = {'x': x_test,'y': y_test,'theta_true': theta_true}\n",
    "            \n",
    "        return env_dict_train, env_dict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "general-instrument",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfig, axs = plt.subplots(1, 2, figsize=(16, 6))\\nax = axs[0]\\nX_train = torch.stack([data_dict_train[e][\\'x\\'] for e in data_dict_train.keys()])\\nX_train = np.array(X_train).reshape(-1,1)\\nY_train = torch.stack([data_dict_train[e][\\'y\\'] for e in data_dict_train.keys()])\\nY_train = np.array(Y_train).reshape(-1,1)\\nax.scatter(y=Y_train, x=X_train, s=3, c=\"grey\", alpha=0.05, label=\"Training envs\")\\nax.legend(loc=\\'upper left\\')\\n\\nax = axs[1]\\nX_test = torch.stack([data_dict_test[e][\\'x\\'] for e in data_dict_test.keys()])\\nX_test = np.array(X_test).reshape(-1,1)\\nY_test = torch.stack([data_dict_test[e][\\'y\\'] for e in data_dict_test.keys()])\\nY_test = np.array(Y_test).reshape(-1,1)\\nax.scatter(y=Y_test, x=X_test, s=3, c=\"grey\", alpha=0.05, label=\"Testing envs\")\\nax.legend(loc=\\'upper left\\')\\nfig.tight_layout()\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Intitialize the experiment and generate the data\n",
    "envs_train, envs_test = 250, 25\n",
    "size_train, size_test = 1000 , 1000\n",
    "theta_dist=\"beta\" \n",
    "rho=\"cvar\"\n",
    "dim = 10\n",
    "data_generator = data_generator(envs_train, envs_test, size_train, size_test, theta_dist, dim)\n",
    "data_dict_train, data_dict_test = data_generator.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sacred-refund",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013423681259155273,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 30,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 19,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2d062c482546db85c74882fb5c0b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Loss: 4.917717933654785\n",
      "Epoch [20/200], Loss: 2.69203782081604\n",
      "Epoch [30/200], Loss: 2.2877681255340576\n",
      "Epoch [40/200], Loss: 2.3545453548431396\n",
      "Epoch [50/200], Loss: 2.318718671798706\n",
      "Epoch [60/200], Loss: 2.2755210399627686\n",
      "Epoch [70/200], Loss: 2.2703981399536133\n",
      "Epoch [80/200], Loss: 2.2653417587280273\n",
      "Epoch [90/200], Loss: 2.258314609527588\n",
      "Epoch [100/200], Loss: 2.2531232833862305\n",
      "Epoch [110/200], Loss: 2.247990369796753\n",
      "Epoch [120/200], Loss: 2.2428994178771973\n",
      "Epoch [130/200], Loss: 2.2380502223968506\n",
      "Epoch [140/200], Loss: 2.2333359718322754\n",
      "Epoch [150/200], Loss: 2.2288055419921875\n",
      "Epoch [160/200], Loss: 2.2244858741760254\n",
      "Epoch [170/200], Loss: 2.2203691005706787\n",
      "Epoch [180/200], Loss: 2.2164645195007324\n",
      "Epoch [190/200], Loss: 2.212777614593506\n",
      "Epoch [200/200], Loss: 2.209333896636963\n",
      "Epoch [10/200], Loss: 5.974839210510254\n",
      "Epoch [20/200], Loss: 3.1057183742523193\n",
      "Epoch [30/200], Loss: 2.372818946838379\n",
      "Epoch [40/200], Loss: 2.405691146850586\n",
      "Epoch [50/200], Loss: 2.407352924346924\n",
      "Epoch [60/200], Loss: 2.3584258556365967\n",
      "Epoch [70/200], Loss: 2.3450212478637695\n",
      "Epoch [80/200], Loss: 2.3431644439697266\n",
      "Epoch [90/200], Loss: 2.3378372192382812\n",
      "Epoch [100/200], Loss: 2.3332574367523193\n",
      "Epoch [110/200], Loss: 2.329444408416748\n",
      "Epoch [120/200], Loss: 2.3254971504211426\n",
      "Epoch [130/200], Loss: 2.321653366088867\n",
      "Epoch [140/200], Loss: 2.317948341369629\n",
      "Epoch [150/200], Loss: 2.3143343925476074\n",
      "Epoch [160/200], Loss: 2.3108410835266113\n",
      "Epoch [170/200], Loss: 2.307483673095703\n",
      "Epoch [180/200], Loss: 2.304260730743408\n",
      "Epoch [190/200], Loss: 2.301177501678467\n",
      "Epoch [200/200], Loss: 2.2982583045959473\n",
      "Epoch [10/200], Loss: 12.381916999816895\n",
      "Epoch [20/200], Loss: 6.668852806091309\n",
      "Epoch [30/200], Loss: 3.6427319049835205\n",
      "Epoch [40/200], Loss: 2.6015024185180664\n",
      "Epoch [50/200], Loss: 2.455122470855713\n",
      "Epoch [60/200], Loss: 2.4799721240997314\n",
      "Epoch [70/200], Loss: 2.474027633666992\n",
      "Epoch [80/200], Loss: 2.455324649810791\n",
      "Epoch [90/200], Loss: 2.4468982219696045\n",
      "Epoch [100/200], Loss: 2.444551944732666\n",
      "Epoch [110/200], Loss: 2.442416191101074\n",
      "Epoch [120/200], Loss: 2.4398531913757324\n",
      "Epoch [130/200], Loss: 2.4374163150787354\n",
      "Epoch [140/200], Loss: 2.435063123703003\n",
      "Epoch [150/200], Loss: 2.432696580886841\n",
      "Epoch [160/200], Loss: 2.43033504486084\n",
      "Epoch [170/200], Loss: 2.428011655807495\n",
      "Epoch [180/200], Loss: 2.4257123470306396\n",
      "Epoch [190/200], Loss: 2.4234464168548584\n",
      "Epoch [200/200], Loss: 2.4212169647216797\n",
      "Epoch [10/200], Loss: 8.903056144714355\n",
      "Epoch [20/200], Loss: 4.522006988525391\n",
      "Epoch [30/200], Loss: 2.8204855918884277\n",
      "Epoch [40/200], Loss: 2.5846376419067383\n",
      "Epoch [50/200], Loss: 2.6379151344299316\n",
      "Epoch [60/200], Loss: 2.611149549484253\n",
      "Epoch [70/200], Loss: 2.5764598846435547\n",
      "Epoch [80/200], Loss: 2.569699764251709\n",
      "Epoch [90/200], Loss: 2.5673835277557373\n",
      "Epoch [100/200], Loss: 2.5628817081451416\n",
      "Epoch [110/200], Loss: 2.5589842796325684\n",
      "Epoch [120/200], Loss: 2.5555269718170166\n",
      "Epoch [130/200], Loss: 2.551975965499878\n",
      "Epoch [140/200], Loss: 2.5484845638275146\n",
      "Epoch [150/200], Loss: 2.5450501441955566\n",
      "Epoch [160/200], Loss: 2.541663408279419\n",
      "Epoch [170/200], Loss: 2.5383400917053223\n",
      "Epoch [180/200], Loss: 2.53513503074646\n",
      "Epoch [190/200], Loss: 2.5320475101470947\n",
      "Epoch [200/200], Loss: 2.5290727615356445\n",
      "Epoch [10/200], Loss: 11.775139808654785\n",
      "Epoch [20/200], Loss: 6.236039161682129\n",
      "Epoch [30/200], Loss: 3.505707263946533\n",
      "Epoch [40/200], Loss: 2.7934505939483643\n",
      "Epoch [50/200], Loss: 2.785412549972534\n",
      "Epoch [60/200], Loss: 2.8026912212371826\n",
      "Epoch [70/200], Loss: 2.76818585395813\n",
      "Epoch [80/200], Loss: 2.748203754425049\n",
      "Epoch [90/200], Loss: 2.7432808876037598\n",
      "Epoch [100/200], Loss: 2.739342451095581\n",
      "Epoch [110/200], Loss: 2.734419822692871\n",
      "Epoch [120/200], Loss: 2.7298038005828857\n",
      "Epoch [130/200], Loss: 2.725320816040039\n",
      "Epoch [140/200], Loss: 2.720825433731079\n",
      "Epoch [150/200], Loss: 2.716327428817749\n",
      "Epoch [160/200], Loss: 2.711858034133911\n",
      "Epoch [170/200], Loss: 2.7074313163757324\n",
      "Epoch [180/200], Loss: 2.7031006813049316\n",
      "Epoch [190/200], Loss: 2.698866128921509\n",
      "Epoch [200/200], Loss: 2.694730281829834\n",
      "Epoch [10/200], Loss: 12.584933280944824\n",
      "Epoch [20/200], Loss: 6.728379726409912\n",
      "Epoch [30/200], Loss: 3.710860013961792\n",
      "Epoch [40/200], Loss: 2.8996188640594482\n",
      "Epoch [50/200], Loss: 2.8877365589141846\n",
      "Epoch [60/200], Loss: 2.910229206085205\n",
      "Epoch [70/200], Loss: 2.8742432594299316\n",
      "Epoch [80/200], Loss: 2.853923797607422\n",
      "Epoch [90/200], Loss: 2.8505308628082275\n",
      "Epoch [100/200], Loss: 2.84798526763916\n",
      "Epoch [110/200], Loss: 2.8443591594696045\n",
      "Epoch [120/200], Loss: 2.840986967086792\n",
      "Epoch [130/200], Loss: 2.8379132747650146\n",
      "Epoch [140/200], Loss: 2.834765911102295\n",
      "Epoch [150/200], Loss: 2.831667184829712\n",
      "Epoch [160/200], Loss: 2.828580856323242\n",
      "Epoch [170/200], Loss: 2.825518846511841\n",
      "Epoch [180/200], Loss: 2.822488784790039\n",
      "Epoch [190/200], Loss: 2.8194987773895264\n",
      "Epoch [200/200], Loss: 2.8165955543518066\n",
      "Epoch [10/200], Loss: 10.711549758911133\n",
      "Epoch [20/200], Loss: 5.529897212982178\n",
      "Epoch [30/200], Loss: 3.3406717777252197\n",
      "Epoch [40/200], Loss: 3.0259151458740234\n",
      "Epoch [50/200], Loss: 3.099194288253784\n",
      "Epoch [60/200], Loss: 3.0638630390167236\n",
      "Epoch [70/200], Loss: 3.016982316970825\n",
      "Epoch [80/200], Loss: 3.011960744857788\n",
      "Epoch [90/200], Loss: 3.0098628997802734\n",
      "Epoch [100/200], Loss: 3.00496244430542\n",
      "Epoch [110/200], Loss: 3.00106143951416\n",
      "Epoch [120/200], Loss: 2.9977056980133057\n",
      "Epoch [130/200], Loss: 2.994321823120117\n",
      "Epoch [140/200], Loss: 2.9909276962280273\n",
      "Epoch [150/200], Loss: 2.9876255989074707\n",
      "Epoch [160/200], Loss: 2.984327554702759\n",
      "Epoch [170/200], Loss: 2.9810829162597656\n",
      "Epoch [180/200], Loss: 2.9778969287872314\n",
      "Epoch [190/200], Loss: 2.97477388381958\n",
      "Epoch [200/200], Loss: 2.971719741821289\n",
      "Epoch [10/200], Loss: 9.910959243774414\n",
      "Epoch [20/200], Loss: 5.048936367034912\n",
      "Epoch [30/200], Loss: 3.3067057132720947\n",
      "Epoch [40/200], Loss: 3.232992649078369\n",
      "Epoch [50/200], Loss: 3.2816295623779297\n",
      "Epoch [60/200], Loss: 3.2133679389953613\n",
      "Epoch [70/200], Loss: 3.174025535583496\n",
      "Epoch [80/200], Loss: 3.173799753189087\n",
      "Epoch [90/200], Loss: 3.1678531169891357\n",
      "Epoch [100/200], Loss: 3.1629621982574463\n",
      "Epoch [110/200], Loss: 3.1595263481140137\n",
      "Epoch [120/200], Loss: 3.1555850505828857\n",
      "Epoch [130/200], Loss: 3.1518912315368652\n",
      "Epoch [140/200], Loss: 3.148178815841675\n",
      "Epoch [150/200], Loss: 3.144505023956299\n",
      "Epoch [160/200], Loss: 3.140869140625\n",
      "Epoch [170/200], Loss: 3.13728404045105\n",
      "Epoch [180/200], Loss: 3.133774995803833\n",
      "Epoch [190/200], Loss: 3.13041615486145\n",
      "Epoch [200/200], Loss: 3.1271708011627197\n",
      "Epoch [10/200], Loss: 9.84623908996582\n",
      "Epoch [20/200], Loss: 5.049968719482422\n",
      "Epoch [30/200], Loss: 3.4647862911224365\n",
      "Epoch [40/200], Loss: 3.4818973541259766\n",
      "Epoch [50/200], Loss: 3.501039505004883\n",
      "Epoch [60/200], Loss: 3.4134347438812256\n",
      "Epoch [70/200], Loss: 3.3939759731292725\n",
      "Epoch [80/200], Loss: 3.393474817276001\n",
      "Epoch [90/200], Loss: 3.3866546154022217\n",
      "Epoch [100/200], Loss: 3.3820412158966064\n",
      "Epoch [110/200], Loss: 3.3782026767730713\n",
      "Epoch [120/200], Loss: 3.3741471767425537\n",
      "Epoch [130/200], Loss: 3.37009596824646\n",
      "Epoch [140/200], Loss: 3.3661091327667236\n",
      "Epoch [150/200], Loss: 3.3621511459350586\n",
      "Epoch [160/200], Loss: 3.3582301139831543\n",
      "Epoch [170/200], Loss: 3.3543648719787598\n",
      "Epoch [180/200], Loss: 3.350567579269409\n",
      "Epoch [190/200], Loss: 3.346848487854004\n",
      "Epoch [200/200], Loss: 3.343217611312866\n",
      "Epoch [10/200], Loss: 12.986705780029297\n",
      "Epoch [20/200], Loss: 6.933559417724609\n",
      "Epoch [30/200], Loss: 4.011163234710693\n",
      "Epoch [40/200], Loss: 3.591942310333252\n",
      "Epoch [50/200], Loss: 3.6888628005981445\n",
      "Epoch [60/200], Loss: 3.6359052658081055\n",
      "Epoch [70/200], Loss: 3.583415985107422\n",
      "Epoch [80/200], Loss: 3.5778892040252686\n",
      "Epoch [90/200], Loss: 3.576925754547119\n",
      "Epoch [100/200], Loss: 3.573270559310913\n",
      "Epoch [110/200], Loss: 3.5697832107543945\n",
      "Epoch [120/200], Loss: 3.5672450065612793\n",
      "Epoch [130/200], Loss: 3.5647075176239014\n",
      "Epoch [140/200], Loss: 3.5621073246002197\n",
      "Epoch [150/200], Loss: 3.5595648288726807\n",
      "Epoch [160/200], Loss: 3.5570287704467773\n",
      "Epoch [170/200], Loss: 3.5545060634613037\n",
      "Epoch [180/200], Loss: 3.552016019821167\n",
      "Epoch [190/200], Loss: 3.5495479106903076\n",
      "Epoch [200/200], Loss: 3.5471153259277344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Loss: 11.825051307678223\n",
      "Epoch [20/200], Loss: 6.186828136444092\n",
      "Epoch [30/200], Loss: 3.9233899116516113\n",
      "Epoch [40/200], Loss: 3.875627279281616\n",
      "Epoch [50/200], Loss: 3.9272377490997314\n",
      "Epoch [60/200], Loss: 3.826097011566162\n",
      "Epoch [70/200], Loss: 3.7983343601226807\n",
      "Epoch [80/200], Loss: 3.8001387119293213\n",
      "Epoch [90/200], Loss: 3.795131206512451\n",
      "Epoch [100/200], Loss: 3.7906839847564697\n",
      "Epoch [110/200], Loss: 3.7879676818847656\n",
      "Epoch [120/200], Loss: 3.785216808319092\n",
      "Epoch [130/200], Loss: 3.782550811767578\n",
      "Epoch [140/200], Loss: 3.77984356880188\n",
      "Epoch [150/200], Loss: 3.7771241664886475\n",
      "Epoch [160/200], Loss: 3.7744345664978027\n",
      "Epoch [170/200], Loss: 3.771763324737549\n",
      "Epoch [180/200], Loss: 3.7691256999969482\n",
      "Epoch [190/200], Loss: 3.766525983810425\n",
      "Epoch [200/200], Loss: 3.7639689445495605\n",
      "Epoch [10/200], Loss: 15.925381660461426\n",
      "Epoch [20/200], Loss: 8.938072204589844\n",
      "Epoch [30/200], Loss: 4.951807975769043\n",
      "Epoch [40/200], Loss: 4.086339950561523\n",
      "Epoch [50/200], Loss: 4.2171549797058105\n",
      "Epoch [60/200], Loss: 4.169353008270264\n",
      "Epoch [70/200], Loss: 4.086378574371338\n",
      "Epoch [80/200], Loss: 4.0782151222229\n",
      "Epoch [90/200], Loss: 4.078125476837158\n",
      "Epoch [100/200], Loss: 4.072958946228027\n",
      "Epoch [110/200], Loss: 4.070429801940918\n",
      "Epoch [120/200], Loss: 4.067925930023193\n",
      "Epoch [130/200], Loss: 4.065221309661865\n",
      "Epoch [140/200], Loss: 4.062572002410889\n",
      "Epoch [150/200], Loss: 4.059975624084473\n",
      "Epoch [160/200], Loss: 4.057366371154785\n",
      "Epoch [170/200], Loss: 4.054762840270996\n",
      "Epoch [180/200], Loss: 4.052166938781738\n",
      "Epoch [190/200], Loss: 4.04958963394165\n",
      "Epoch [200/200], Loss: 4.0470380783081055\n",
      "Epoch [10/200], Loss: 21.790653228759766\n",
      "Epoch [20/200], Loss: 13.257242202758789\n",
      "Epoch [30/200], Loss: 7.476666450500488\n",
      "Epoch [40/200], Loss: 4.755128860473633\n",
      "Epoch [50/200], Loss: 4.42712926864624\n",
      "Epoch [60/200], Loss: 4.527522563934326\n",
      "Epoch [70/200], Loss: 4.451016426086426\n",
      "Epoch [80/200], Loss: 4.3912811279296875\n",
      "Epoch [90/200], Loss: 4.391342639923096\n",
      "Epoch [100/200], Loss: 4.387506484985352\n",
      "Epoch [110/200], Loss: 4.381019115447998\n",
      "Epoch [120/200], Loss: 4.378593921661377\n",
      "Epoch [130/200], Loss: 4.375299453735352\n",
      "Epoch [140/200], Loss: 4.372442245483398\n",
      "Epoch [150/200], Loss: 4.369533061981201\n",
      "Epoch [160/200], Loss: 4.366586685180664\n",
      "Epoch [170/200], Loss: 4.363637447357178\n",
      "Epoch [180/200], Loss: 4.360687732696533\n",
      "Epoch [190/200], Loss: 4.357738494873047\n",
      "Epoch [200/200], Loss: 4.354794025421143\n",
      "Epoch [10/200], Loss: 17.689226150512695\n",
      "Epoch [20/200], Loss: 10.156259536743164\n",
      "Epoch [30/200], Loss: 5.663562297821045\n",
      "Epoch [40/200], Loss: 4.647087097167969\n",
      "Epoch [50/200], Loss: 4.85200309753418\n",
      "Epoch [60/200], Loss: 4.725389003753662\n",
      "Epoch [70/200], Loss: 4.64145565032959\n",
      "Epoch [80/200], Loss: 4.653827667236328\n",
      "Epoch [90/200], Loss: 4.642000198364258\n",
      "Epoch [100/200], Loss: 4.640231132507324\n",
      "Epoch [110/200], Loss: 4.63979434967041\n",
      "Epoch [120/200], Loss: 4.638524532318115\n",
      "Epoch [130/200], Loss: 4.638122081756592\n",
      "Epoch [140/200], Loss: 4.637549877166748\n",
      "Epoch [150/200], Loss: 4.636964797973633\n",
      "Epoch [160/200], Loss: 4.636430740356445\n",
      "Epoch [170/200], Loss: 4.635913848876953\n",
      "Epoch [180/200], Loss: 4.635384559631348\n",
      "Epoch [190/200], Loss: 4.634866714477539\n",
      "Epoch [200/200], Loss: 4.634350776672363\n",
      "Epoch [10/200], Loss: 18.368860244750977\n",
      "Epoch [20/200], Loss: 10.64272403717041\n",
      "Epoch [30/200], Loss: 5.96471643447876\n",
      "Epoch [40/200], Loss: 5.095544338226318\n",
      "Epoch [50/200], Loss: 5.29204797744751\n",
      "Epoch [60/200], Loss: 5.143754005432129\n",
      "Epoch [70/200], Loss: 5.062032699584961\n",
      "Epoch [80/200], Loss: 5.067736625671387\n",
      "Epoch [90/200], Loss: 5.063432693481445\n",
      "Epoch [100/200], Loss: 5.0581488609313965\n",
      "Epoch [110/200], Loss: 5.057216644287109\n",
      "Epoch [120/200], Loss: 5.055750370025635\n",
      "Epoch [130/200], Loss: 5.054447650909424\n",
      "Epoch [140/200], Loss: 5.05343770980835\n",
      "Epoch [150/200], Loss: 5.052401542663574\n",
      "Epoch [160/200], Loss: 5.051358222961426\n",
      "Epoch [170/200], Loss: 5.050304889678955\n",
      "Epoch [180/200], Loss: 5.049290180206299\n",
      "Epoch [190/200], Loss: 5.048233985900879\n",
      "Epoch [200/200], Loss: 5.047219753265381\n",
      "Epoch [10/200], Loss: 19.64561653137207\n",
      "Epoch [20/200], Loss: 11.581766128540039\n",
      "Epoch [30/200], Loss: 6.567950248718262\n",
      "Epoch [40/200], Loss: 5.644657611846924\n",
      "Epoch [50/200], Loss: 5.861516952514648\n",
      "Epoch [60/200], Loss: 5.657094955444336\n",
      "Epoch [70/200], Loss: 5.5938897132873535\n",
      "Epoch [80/200], Loss: 5.602566719055176\n",
      "Epoch [90/200], Loss: 5.590832710266113\n",
      "Epoch [100/200], Loss: 5.588165283203125\n",
      "Epoch [110/200], Loss: 5.586338043212891\n",
      "Epoch [120/200], Loss: 5.584209442138672\n",
      "Epoch [130/200], Loss: 5.582523822784424\n",
      "Epoch [140/200], Loss: 5.580855846405029\n",
      "Epoch [150/200], Loss: 5.5791335105896\n",
      "Epoch [160/200], Loss: 5.57741641998291\n",
      "Epoch [170/200], Loss: 5.575717926025391\n",
      "Epoch [180/200], Loss: 5.574000835418701\n",
      "Epoch [190/200], Loss: 5.572300434112549\n",
      "Epoch [200/200], Loss: 5.570600986480713\n",
      "Epoch [10/200], Loss: 20.307968139648438\n",
      "Epoch [20/200], Loss: 12.056073188781738\n",
      "Epoch [30/200], Loss: 6.956965446472168\n",
      "Epoch [40/200], Loss: 6.264960765838623\n",
      "Epoch [50/200], Loss: 6.434159755706787\n",
      "Epoch [60/200], Loss: 6.144700050354004\n",
      "Epoch [70/200], Loss: 6.183341979980469\n",
      "Epoch [80/200], Loss: 6.154836654663086\n",
      "Epoch [90/200], Loss: 6.146582126617432\n",
      "Epoch [100/200], Loss: 6.139759540557861\n",
      "Epoch [110/200], Loss: 6.139161109924316\n",
      "Epoch [120/200], Loss: 6.1371684074401855\n",
      "Epoch [130/200], Loss: 6.135555267333984\n",
      "Epoch [140/200], Loss: 6.134093761444092\n",
      "Epoch [150/200], Loss: 6.132656097412109\n",
      "Epoch [160/200], Loss: 6.1311774253845215\n",
      "Epoch [170/200], Loss: 6.129696369171143\n",
      "Epoch [180/200], Loss: 6.128223419189453\n",
      "Epoch [190/200], Loss: 6.126736640930176\n",
      "Epoch [200/200], Loss: 6.12526798248291\n",
      "Epoch [10/200], Loss: 18.81133270263672\n",
      "Epoch [20/200], Loss: 10.925366401672363\n",
      "Epoch [30/200], Loss: 7.011780261993408\n",
      "Epoch [40/200], Loss: 7.486598968505859\n",
      "Epoch [50/200], Loss: 7.078835487365723\n",
      "Epoch [60/200], Loss: 7.00954532623291\n",
      "Epoch [70/200], Loss: 6.998031139373779\n",
      "Epoch [80/200], Loss: 6.97396993637085\n",
      "Epoch [90/200], Loss: 6.972139835357666\n",
      "Epoch [100/200], Loss: 6.969809055328369\n",
      "Epoch [110/200], Loss: 6.967944145202637\n",
      "Epoch [120/200], Loss: 6.965816497802734\n",
      "Epoch [130/200], Loss: 6.963608264923096\n",
      "Epoch [140/200], Loss: 6.961423397064209\n",
      "Epoch [150/200], Loss: 6.959250450134277\n",
      "Epoch [160/200], Loss: 6.957067966461182\n",
      "Epoch [170/200], Loss: 6.954875946044922\n",
      "Epoch [180/200], Loss: 6.952682971954346\n",
      "Epoch [190/200], Loss: 6.950495719909668\n",
      "Epoch [200/200], Loss: 6.9483184814453125\n",
      "Epoch [10/200], Loss: 29.0770263671875\n",
      "Epoch [20/200], Loss: 18.792787551879883\n",
      "Epoch [30/200], Loss: 11.317607879638672\n",
      "Epoch [40/200], Loss: 8.068613052368164\n",
      "Epoch [50/200], Loss: 8.617487907409668\n",
      "Epoch [60/200], Loss: 8.07329273223877\n",
      "Epoch [70/200], Loss: 8.119698524475098\n",
      "Epoch [80/200], Loss: 8.06417179107666\n",
      "Epoch [90/200], Loss: 8.060711860656738\n",
      "Epoch [100/200], Loss: 8.053335189819336\n",
      "Epoch [110/200], Loss: 8.052957534790039\n",
      "Epoch [120/200], Loss: 8.048748016357422\n",
      "Epoch [130/200], Loss: 8.04781436920166\n",
      "Epoch [140/200], Loss: 8.046103477478027\n",
      "Epoch [150/200], Loss: 8.044800758361816\n",
      "Epoch [160/200], Loss: 8.043455123901367\n",
      "Epoch [170/200], Loss: 8.041621208190918\n",
      "Epoch [180/200], Loss: 8.04043197631836\n",
      "Epoch [190/200], Loss: 8.039112091064453\n",
      "Epoch [200/200], Loss: 8.03756332397461\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Run the ARM regression task\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "results = pd.DataFrame(columns = [\"alpha\", \"theta_hat\", \"cvar\"])\n",
    "fig, ax = plt.subplots(1, 1, sharey=True, figsize=(8,6))\n",
    "rg = range(5,100,5)\n",
    "diverging_colors = sns.color_palette(\"RdBu\", len(rg))\n",
    "j=1\n",
    "\n",
    "for i in tqdm(rg):\n",
    "    f = FHatNetwork(dim)\n",
    "    ARM_Regression(name=rho).fit(f, data_dict_train, i/100)  \n",
    "    with torch.no_grad():\n",
    "        risks = torch.stack([loss_fn(data_dict_test[e]['y'], f(data_dict_test[e]['x'])) for e in data_dict_test.keys()])\n",
    "        cvar_emp = aggregation_function(name=\"cvar\").aggregate(risks, i/100)     \n",
    "        sns.kdeplot(risks.numpy(), ax=ax,color=diverging_colors[len(rg)-j], label=str(i/100))\n",
    "    \n",
    "    results.at[i, \"alpha\"] = i/100\n",
    "    results.at[i, \"theta_hat\"] = f.state_dict()\n",
    "    results.at[i, \"cvar\"] = cvar_emp\n",
    "    j+=1\n",
    "    del(f)\n",
    "ax.legend(bbox_to_anchor=(1.02, 1.02), title=r\"$\\alpha$\", loc='upper left')\n",
    "#ax.set_title(r\"Distribution of the risks on the training data ($\\alpha$)\")\n",
    "ax.set_title(r\"Dist of risks on val for independent optimization of $f_\\theta$\")\n",
    "norm = plt.Normalize(0.05, 0.95)\n",
    "sm = plt.cm.ScalarMappable(cmap=\"RdBu_r\", norm=norm)\n",
    "sm.set_array([])\n",
    "\n",
    "ax.get_legend().remove()\n",
    "ax.figure.colorbar(sm).set_label(label= r'$\\alpha$',labelpad=5,size=12)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3423433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha tensor([0.]) torch.Size([1])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot assign 'torch.FloatTensor' as parameter 'weight' (torch.nn.Parameter or None expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m diverging_colors \u001b[38;5;241m=\u001b[39m sns\u001b[38;5;241m.\u001b[39mcolor_palette(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRdBu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(rg))\n\u001b[1;32m      8\u001b[0m j\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mARM_Regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrho\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_independent_h_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dict_train\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     11\u001b[0m h\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(rg):\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mARM_Regression.fit_independent_h_loop\u001b[0;34m(self, h, env_dict)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0.05\u001b[39m):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs): \n\u001b[0;32m---> 54\u001b[0m         risks \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([loss_fn(env_dict[e][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m],h(env_dict[e][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m], alpha)) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m env_dict\u001b[38;5;241m.\u001b[39mkeys()])\n\u001b[1;32m     55\u001b[0m         cvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregator\u001b[38;5;241m.\u001b[39maggregate(risks, alpha)\n\u001b[1;32m     56\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0.05\u001b[39m):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs): \n\u001b[0;32m---> 54\u001b[0m         risks \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([loss_fn(env_dict[e][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43me\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m env_dict\u001b[38;5;241m.\u001b[39mkeys()])\n\u001b[1;32m     55\u001b[0m         cvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregator\u001b[38;5;241m.\u001b[39maggregate(risks, alpha)\n\u001b[1;32m     56\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1129\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1128\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mHNetwork.forward\u001b[0;34m(self, x, alpha)\u001b[0m\n\u001b[1;32m     28\u001b[0m alpha \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([alpha])\n\u001b[1;32m     29\u001b[0m theta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_network(alpha)\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_network\u001b[38;5;241m.\u001b[39mlinear\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m theta\n\u001b[1;32m     31\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_network(x)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1225\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1223\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m   1224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1225\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as parameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1226\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(torch.nn.Parameter or None expected)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1227\u001b[0m                         \u001b[38;5;241m.\u001b[39mformat(torch\u001b[38;5;241m.\u001b[39mtypename(value), name))\n\u001b[1;32m   1228\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(name, value)\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot assign 'torch.FloatTensor' as parameter 'weight' (torch.nn.Parameter or None expected)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Run the ARM regression task\n",
    "h = HNetwork(dim)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "results = pd.DataFrame(columns = [\"alpha\", \"theta_hat\", \"cvar\"])\n",
    "fig, ax = plt.subplots(1, 1, sharey=True, figsize=(8,6))\n",
    "rg = range(5,100,5)\n",
    "diverging_colors = sns.color_palette(\"RdBu\", len(rg))\n",
    "j=1\n",
    "\n",
    "ARM_Regression(name=rho).fit_independent_h_loop(h, data_dict_train)  \n",
    "h.eval()\n",
    "for i in tqdm(rg):\n",
    "    risks = torch.stack([loss_fn(data_dict_test[e]['y'], h(data_dict_test[e]['x'],i/100)) for e in data_dict_test.keys()])\n",
    "    cvar_emp = aggregation_function(name=\"cvar\").aggregate(risks, i/100)\n",
    "             \n",
    "    sns.kdeplot(np.array(risks), ax=ax,color=diverging_colors[len(rg)-j], label=str(i/100))\n",
    "    \n",
    "    results.at[i, \"alpha\"] = i/100\n",
    "    results.at[i, \"theta_hat\"] = theta[0]\n",
    "    results.at[i, \"cvar\"] = cvar_emp\n",
    "    j+=1\n",
    "    \n",
    "ax.legend(bbox_to_anchor=(1.02, 1.02), title=r\"$\\alpha$\", loc='upper left')\n",
    "ax.set_title(r\"Dist of risks on val for iterative independent optimization of $h_\\beta(x,\\alpha')$\")\n",
    "norm = plt.Normalize(0.05, 0.95)\n",
    "sm = plt.cm.ScalarMappable(cmap=\"RdBu_r\", norm=norm)\n",
    "sm.set_array([])\n",
    "\n",
    "ax.get_legend().remove()\n",
    "ax.figure.colorbar(sm).set_label(label= r'$\\alpha$',labelpad=5,size=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "del(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1a265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the ARM regression task\n",
    "h = HNetwork(dim)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "results = pd.DataFrame(columns = [\"alpha\", \"theta_hat\", \"cvar\"])\n",
    "fig, ax = plt.subplots(1, 1, sharey=True, figsize=(8,6))\n",
    "rg = range(5,100,5)\n",
    "diverging_colors = sns.color_palette(\"RdBu\", len(rg))\n",
    "j=1\n",
    "ARM_Regression(name=rho).fit_independent_h_probablistic(h, data_dict_train)  \n",
    "h.eval()\n",
    "for i in tqdm(rg):\n",
    "    risks = torch.stack([loss_fn(data_dict_test[e]['y'], h(data_dict_test[e]['x'],i/100)) for e in data_dict_test.keys()])\n",
    "    cvar_emp = aggregation_function(name=\"cvar\").aggregate(risks, i/100)\n",
    "             \n",
    "    sns.kdeplot(np.array(risks), ax=ax,color=diverging_colors[len(rg)-j], label=str(i/100))\n",
    "    \n",
    "    results.at[i, \"alpha\"] = i/100\n",
    "    results.at[i, \"theta_hat\"] = theta[0]\n",
    "    results.at[i, \"cvar\"] = cvar_emp\n",
    "    j+=1\n",
    "    \n",
    "ax.legend(bbox_to_anchor=(1.02, 1.02), title=r\"$\\alpha$\", loc='upper left')\n",
    "ax.set_title(r\"Dist of risks on val for independent optimization of $h_\\beta(x,\\alpha')$ $\\alpha' \\sim U[0,1]$\")\n",
    "norm = plt.Normalize(0.05, 0.95)\n",
    "sm = plt.cm.ScalarMappable(cmap=\"RdBu_r\", norm=norm)\n",
    "sm.set_array([])\n",
    "\n",
    "ax.get_legend().remove()\n",
    "ax.figure.colorbar(sm).set_label(label= r'$\\alpha$',labelpad=5,size=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "del(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca2938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the ARM regression task\n",
    "h = HNetwork(dim)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "results = pd.DataFrame(columns = [\"alpha\", \"theta_hat\", \"cvar\"])\n",
    "fig, ax = plt.subplots(1, 1, sharey=True, figsize=(8,6))\n",
    "rg = range(5,100,5)\n",
    "diverging_colors = sns.color_palette(\"RdBu\", len(rg))\n",
    "j=1\n",
    "ARM_Regression(name=rho).fit_h(h, data_dict_train) \n",
    "h.eval()\n",
    "for i in tqdm(rg):\n",
    "    risks = torch.stack([loss_fn(data_dict_test[e]['y'], h(data_dict_test[e]['x'],i/100)) for e in data_dict_test.keys()])\n",
    "    cvar_emp = aggregation_function(name=\"cvar\").aggregate(risks, i/100)\n",
    "             \n",
    "    sns.kdeplot(np.array(risks), ax=ax,color=diverging_colors[len(rg)-j], label=str(i/100))\n",
    "    \n",
    "    results.at[i, \"alpha\"] = i/100\n",
    "    results.at[i, \"theta_hat\"] = theta[0]\n",
    "    results.at[i, \"cvar\"] = cvar_emp\n",
    "    j+=1\n",
    "    \n",
    "ax.legend(bbox_to_anchor=(1.02, 1.02), title=r\"$\\alpha$\", loc='upper left')\n",
    "ax.set_title(r\"Dist of risks on val for direct optimization (Our Method)\")\n",
    "\n",
    "norm = plt.Normalize(0.05, 0.95)\n",
    "sm = plt.cm.ScalarMappable(cmap=\"RdBu_r\", norm=norm)\n",
    "sm.set_array([])\n",
    "\n",
    "ax.get_legend().remove()\n",
    "ax.figure.colorbar(sm).set_label(label= r'$\\alpha$',labelpad=5,size=12)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-maine",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plot the CVAR landscape and the estimated thetas\n",
    "\n",
    "step_size= 0.1\n",
    "rg_alpha = np.arange(0, 1, step_size)\n",
    "rg_theta = np.arange(0, 1, step_size)\n",
    "output_list = list(itertools.product(*[rg_alpha, rg_theta]))\n",
    "\n",
    "def array_for_heatmap(data_dict, alpha, theta):\n",
    "\n",
    "    risks = [mean_squared_error(data_dict[e]['y'], f(data_dict[e]['x'], theta)) for e in data_dict.keys()]\n",
    "    cvar = aggregation_function(name=\"cvar\").aggregate(risks, alpha*100)\n",
    "    \n",
    "    return [np.round(alpha, 3), np.round(theta, 3), np.round(cvar, 3)]\n",
    "\n",
    "temp = Parallel(n_jobs=8)(delayed(array_for_heatmap)(data_dict, j[0], j[1]) for j in tqdm(output_list))\n",
    "\n",
    "# Prepare the data for the landscape plot\n",
    "index, columns = np.unique([i[0] for i in temp]), np.unique([i[0] for i in temp])\n",
    "\n",
    "landscape = pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "for i in temp:\n",
    "    landscape.at[i[0], i[1]] = float(i[2]) #if not np.nan else np.nan\n",
    "    \n",
    "landscape = landscape.sort_index(ascending=False).apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "del temp\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
    "\n",
    "vmin, vmax=0.0, 1.5\n",
    "\n",
    "img = ax.imshow(landscape,extent=[0,1,0,1], interpolation='bilinear', cmap=\"Greens\", vmin=vmin, vmax=vmax) #extent=[0,1,0,1]\n",
    "\n",
    "plt.xlabel(r'$\\Theta$', fontsize = 16)\n",
    "plt.ylabel(r'$\\alpha$', fontsize = 16)\n",
    "img = ax.scatter(y=results.alpha, x=results.theta_hat, c = results.cvar, edgecolors='black', cmap = 'Greens', vmin=vmin, vmax=vmax, label=\"Independent\")\n",
    "ax.set_title(r\"Landscape for $CVaR_{\\alpha}(\\theta)$ and circles represent $\\hat{\\theta}$\")\n",
    "fig.colorbar(img).set_label(label='CVaR value',labelpad=5,size=12)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-criminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the data and the estimated regression function\n",
    "\n",
    "j=1\n",
    "fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
    "\n",
    "X_train = [data_dict[e]['x'].ravel() for e in data_dict.keys()]\n",
    "X_train = np.array([item for sublist in X_train for item in sublist]).reshape(-1,1)\n",
    "Y_train = [data_dict[e]['y'] for e in data_dict.keys()]\n",
    "Y_train = np.array([item for sublist in Y_train for item in sublist]).reshape(-1,1)\n",
    "\n",
    "ax.scatter(y=Y_train, x=X_train, s=3, c=\"grey\", alpha=0.05, label=\"Training envs\")\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "diverging_colors = sns.color_palette(\"RdBu\", len(results))\n",
    "\n",
    "for index, row in results.iterrows():\n",
    "\n",
    "    x_plt = np.arange(0,4,0.1)\n",
    "    ax.plot(x_plt, f(x_plt, row['theta_hat']), c =diverging_colors[len(results)-j], alpha=0.5, label=\"Test envs\")\n",
    "    j+=1\n",
    "\n",
    "norm = plt.Normalize(0.05, 0.95)\n",
    "sm = plt.cm.ScalarMappable(cmap=\"RdBu_r\", norm=norm)\n",
    "sm.set_array([])\n",
    "\n",
    "ax.get_legend().remove()\n",
    "ax.figure.colorbar(sm).set_label(label= r'$\\alpha$',labelpad=5,size=12)\n",
    "\n",
    "ax.axis(xmin=0,xmax=3)\n",
    "ax.axis(xmin=0,xmax=3)\n",
    "\n",
    "ax.set_title(\"Generated data\")\n",
    "ax.set_xlabel(r\"$X$\")\n",
    "ax.set_ylabel(r\"$Y$\")\n",
    "\n",
    "ax.set_xlabel(r\"$X$\")\n",
    "ax.set_title(r\"Estimated regression functions $h(\\alpha,x)$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b38d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "alpha_plt = np.arange(0, 1, 0.05)\n",
    "x_plt = np.arange(4, 6, 0.01)\n",
    "\n",
    "h = np.array([f(x,theta) for theta in list(results['theta_hat']) for x in x_plt]).reshape(len(alpha_plt), len(x_plt))\n",
    "\n",
    "plt.figure()\n",
    "plt.pcolor(x_plt, alpha_plt, h, cmap=plt.get_cmap('RdPu'))\n",
    "contour = plt.contour(x_plt, alpha_plt, h, colors='black', levels=10)  # Add contour lines\n",
    "plt.xlabel(r\"$X$\")\n",
    "plt.ylabel(r\"$\\alpha$\")\n",
    "plt.title(r\"$h(X,\\alpha)$\")\n",
    "plt.colorbar(contour)  # Add color to the color bar\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-round",
   "metadata": {},
   "source": [
    "**Experiment 2** We assume the following *linear* data generation process $$Y(X_{1},X_{2}) = X_{1}^{2}*\\theta_{1,e}+X_{2}*\\theta_{2,e}+\\epsilon$$ and the following non-linear data generation process\n",
    "\n",
    "$$Y(X_{1},X_{2}) = \\exp^{-\\sqrt{X_{1}*\\theta_{1,e}+X_{2}*\\theta_{2,e}}}+\\epsilon$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dada91",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = h_hat(experiment)\n",
    "results = pd.DataFrame(columns = [\"alpha\", \"theta_hat\", \"cvar\"])\n",
    "fig, ax = plt.subplots(1, 1, sharey=True, figsize=(8,6))\n",
    "alpha_hat = range(1,100,1)\n",
    "diverging_colors = sns.color_palette(\"RdBu\", len(alpha_hat))\n",
    "j=1\n",
    "\n",
    "for i in tqdm(alpha_hat):\n",
    "    f = lambda x, theta: h(x,theta,i)\n",
    "    theta_hat = ARM_Regression(name=rho, experiment=experiment).fith(f, data_dict, i)  \n",
    "    risks = [mean_squared_error(data_dict[e]['y'], f(data_dict[e]['x'], theta_hat)) for e in data_dict.keys()]\n",
    "    cvar_emp = aggregation_function(name=\"cvar\").aggregate(risks, i)\n",
    "    \n",
    "    sns.kdeplot(risks, ax=ax,color=diverging_colors[len(alpha_hat)-j], label=str(i/100))\n",
    "    \n",
    "    results.at[i, \"alpha\"] = i/100\n",
    "    results.at[i, \"theta_hat\"] = theta_hat[0]\n",
    "    results.at[i, \"cvar\"] = cvar_emp\n",
    "    j+=1\n",
    "    \n",
    "ax.legend(bbox_to_anchor=(1.02, 1.02), title=r\"$\\alpha$\", loc='upper left')\n",
    "ax.set_title(r\"Distribution of the risks on the training data ($\\alpha$)\")\n",
    "\n",
    "norm = plt.Normalize(0.05, 0.95)\n",
    "sm = plt.cm.ScalarMappable(cmap=\"RdBu_r\", norm=norm)\n",
    "sm.set_array([])\n",
    "\n",
    "ax.get_legend().remove()\n",
    "ax.figure.colorbar(sm).set_label(label= r'$\\alpha$',labelpad=5,size=12)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d3d4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
